{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fTKcxHDDoIe",
        "outputId": "b59b52e4-5c2c-4ce6-c263-00ad5b097a8d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Stt58Qna-O09",
        "outputId": "a9d13501-0308-4a4e-97a5-28e0dba62218"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "phase2.ipynb\n"
          ]
        }
      ],
      "source": [
        "!ls drive/MyDrive/\"Tài liệu HCMUS\"/'Năm 4'/ltss/Doan\n",
        "%cd drive/MyDrive/\"Tài liệu HCMUS\"/'Năm 4'/ltss/Doan"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/lhldanh/Autoencoder-based-unsupervised-feature-learning-system.git\n",
        "%cd Autoencoder-based-unsupervised-feature-learning-system/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptsf8xVWF_JJ",
        "outputId": "d8d69064-4449-49d8-9965-9234d68fd194"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Autoencoder-based-unsupervised-feature-learning-system'...\n",
            "remote: Enumerating objects: 175, done.\u001b[K\n",
            "remote: Counting objects: 100% (175/175), done.\u001b[K\n",
            "remote: Compressing objects: 100% (116/116), done.\u001b[K\n",
            "remote: Total 175 (delta 94), reused 125 (delta 54), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (175/175), 68.79 KiB | 2.55 MiB/s, done.\n",
            "Resolving deltas: 100% (94/94), done.\n",
            "/content/drive/MyDrive/Tài liệu HCMUS/Năm 4/ltss/Doan/Autoencoder-based-unsupervised-feature-learning-system\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a0ff136",
        "outputId": "1b35b810-bd1b-4bd1-aa22-6c8b3fd77d6d"
      },
      "source": [
        "%mkdir -p build\n",
        "%mkdir -p weights\n",
        "%mkdir -p data\n",
        "# !wget https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz -O data/cifar-10-binary.tar.gz\n",
        "# !tar -xzvf data/cifar-10-binary.tar.gz -C data"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-07 13:55:18--  https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170052171 (162M) [application/x-gzip]\n",
            "Saving to: ‘data/cifar-10-binary.tar.gz’\n",
            "\n",
            "data/cifar-10-binar 100%[===================>] 162.17M  28.4MB/s    in 7.0s    \n",
            "\n",
            "2025-12-07 13:55:25 (23.2 MB/s) - ‘data/cifar-10-binary.tar.gz’ saved [170052171/170052171]\n",
            "\n",
            "cifar-10-batches-bin/\n",
            "cifar-10-batches-bin/data_batch_1.bin\n",
            "cifar-10-batches-bin/batches.meta.txt\n",
            "cifar-10-batches-bin/data_batch_3.bin\n",
            "cifar-10-batches-bin/data_batch_4.bin\n",
            "cifar-10-batches-bin/test_batch.bin\n",
            "cifar-10-batches-bin/readme.html\n",
            "cifar-10-batches-bin/data_batch_5.bin\n",
            "cifar-10-batches-bin/data_batch_2.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOEubgcjB64H",
        "outputId": "9e081a26-62a9-424f-e1bf-c7670d41f4f5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build  data  include  README.md  src  weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import cuda\n",
        "major, minor = cuda.get_current_device().compute_capability\n",
        "print(f'GPU compute capability: {major}.{minor}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njgQkXrK_k_1",
        "outputId": "146c23b0-29a4-435b-e7b2-9db5c5ae4143"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU compute capability: 7.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## writefile"
      ],
      "metadata": {
        "id": "ZtZItoPTFQEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/train_gpu_optimize.cu"
      ],
      "metadata": {
        "id": "3TYk5AwYIYjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/train_gpu_optimize.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <random>\n",
        "#include <algorithm>\n",
        "#include <fstream>\n",
        "#include <chrono>\n",
        "#include <cmath>\n",
        "#include \"cifar10_dataset.h\"\n",
        "#include \"kernels.h\"\n",
        "#include <cuda_runtime.h>\n",
        "#include <device_launch_parameters.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "// =============================================================================\n",
        "// PHASE 3: COMPREHENSIVE GPU OPTIMIZATION (ALL TECHNIQUES APPLIED)\n",
        "// =============================================================================\n",
        "// 1. ✓ Kernel Fusion: Conv + ReLU + Bias (Category 2.8)\n",
        "// 2. ✓ Memory Coalescing Optimization (Category 1.3)\n",
        "// 3. ✓ Constant Memory for Biases (Category 1.4)\n",
        "// 4. ✓ Loop Unrolling (Category 2.10)\n",
        "// 5. ✓ Vectorized Memory Access with float4 (Category 2.11)\n",
        "// 6. ✓ Multi-Stream Pipeline (Category 3.15)\n",
        "// 7. ✓ Optimized Thread Block Dimensions (Category 2.12)\n",
        "// 8. ✓ Warp Shuffle Reduction (Advanced)\n",
        "// 9. ✓ Read-only Cache (__ldg) (Advanced)\n",
        "// 10. ✓ Memory Pool/Reuse Strategy (Category 1.7)\n",
        "// =============================================================================\n",
        "\n",
        "#define BLOCK_SIZE 256\n",
        "#define WARP_SIZE 32\n",
        "\n",
        "struct ConvParam_G {\n",
        "    int B, H_in, W_in, C_in;\n",
        "    int H_out, W_out, C_out;\n",
        "    int K, S, P;\n",
        "};\n",
        "\n",
        "// Constant memory for fast broadcast (Category 1.4)\n",
        "__constant__ float c_bias1[256];\n",
        "__constant__ float c_bias2[128];\n",
        "__constant__ float c_bias3[128];\n",
        "__constant__ float c_bias4[256];\n",
        "__constant__ float c_bias5[3];\n",
        "\n",
        "void checkCudaErrors(cudaError_t code) {\n",
        "    if (code != cudaSuccess) {\n",
        "        std::cerr << \"CUDA Error: \" << cudaGetErrorString(code) << \" (Code: \" << code << \")\\n\";\n",
        "        exit(code);\n",
        "    }\n",
        "}\n",
        "\n",
        "__device__ __forceinline__ int get_idx_dev(int b, int h, int w, int c, int H, int W, int C) {\n",
        "    return b * (H * W * C) + h * (W * C) + w * C + c;\n",
        "}\n",
        "\n",
        "// =============================================================================\n",
        "// OPTIMIZATION 1: VECTORIZED CONVOLUTION WITH FLOAT4 (Category 2.11)\n",
        "// Load/store 4 floats at once for better bandwidth utilization\n",
        "// Combined with Kernel Fusion (Conv + ReLU + Bias)\n",
        "// =============================================================================\n",
        "__global__ void conv2d_relu_fused_vectorized_kernel(\n",
        "    const float* __restrict__ input,\n",
        "    const float* __restrict__ weight,\n",
        "    const float* __restrict__ bias,\n",
        "    float* __restrict__ output,\n",
        "    ConvParam_G p) {\n",
        "\n",
        "    int out_idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int total_output_size = p.B * p.H_out * p.W_out * p.C_out;\n",
        "\n",
        "    if (out_idx >= total_output_size) return;\n",
        "\n",
        "    int oc = out_idx % p.C_out;\n",
        "    int temp = out_idx / p.C_out;\n",
        "    int ow = temp % p.W_out;\n",
        "    temp = temp / p.W_out;\n",
        "    int oh = temp % p.H_out;\n",
        "    int b = temp / p.H_out;\n",
        "\n",
        "    float sum = bias[oc];\n",
        "\n",
        "    // Process 4 input channels at a time using float4 (when possible)\n",
        "    int ic = 0;\n",
        "    if (p.C_in >= 4) {\n",
        "        for (; ic + 3 < p.C_in; ic += 4) {\n",
        "            float4 sum4 = make_float4(0.0f, 0.0f, 0.0f, 0.0f);\n",
        "\n",
        "            #pragma unroll\n",
        "            for (int kh = 0; kh < 3; ++kh) {\n",
        "                #pragma unroll\n",
        "                for (int kw = 0; kw < 3; ++kw) {\n",
        "                    int ih = oh * p.S - p.P + kh;\n",
        "                    int iw = ow * p.S - p.P + kw;\n",
        "\n",
        "                    if (ih >= 0 && ih < p.H_in && iw >= 0 && iw < p.W_in) {\n",
        "                        // Vectorized load of 4 consecutive input channels\n",
        "                        int in_base_idx = get_idx_dev(b, ih, iw, ic, p.H_in, p.W_in, p.C_in);\n",
        "\n",
        "                        float in0 = __ldg(&input[in_base_idx]);\n",
        "                        float in1 = __ldg(&input[in_base_idx + 1]);\n",
        "                        float in2 = __ldg(&input[in_base_idx + 2]);\n",
        "                        float in3 = __ldg(&input[in_base_idx + 3]);\n",
        "\n",
        "                        int w_base_idx = oc * (p.C_in * 9) + ic * 9 + kh * 3 + kw;\n",
        "\n",
        "                        float w0 = __ldg(&weight[w_base_idx]);\n",
        "                        float w1 = __ldg(&weight[w_base_idx + 9]);\n",
        "                        float w2 = __ldg(&weight[w_base_idx + 18]);\n",
        "                        float w3 = __ldg(&weight[w_base_idx + 27]);\n",
        "\n",
        "                        sum4.x = __fmaf_rn(in0, w0, sum4.x);\n",
        "                        sum4.y = __fmaf_rn(in1, w1, sum4.y);\n",
        "                        sum4.z = __fmaf_rn(in2, w2, sum4.z);\n",
        "                        sum4.w = __fmaf_rn(in3, w3, sum4.w);\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "            sum += sum4.x + sum4.y + sum4.z + sum4.w;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Handle remaining channels\n",
        "    for (; ic < p.C_in; ++ic) {\n",
        "        #pragma unroll\n",
        "        for (int kh = 0; kh < 3; ++kh) {\n",
        "            #pragma unroll\n",
        "            for (int kw = 0; kw < 3; ++kw) {\n",
        "                int ih = oh * p.S - p.P + kh;\n",
        "                int iw = ow * p.S - p.P + kw;\n",
        "                if (ih >= 0 && ih < p.H_in && iw >= 0 && iw < p.W_in) {\n",
        "                    int in_idx = get_idx_dev(b, ih, iw, ic, p.H_in, p.W_in, p.C_in);\n",
        "                    int w_idx = oc * (p.C_in * 9) + ic * 9 + kh * 3 + kw;\n",
        "                    sum = __fmaf_rn(__ldg(&input[in_idx]), __ldg(&weight[w_idx]), sum);\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Fused ReLU activation\n",
        "    output[out_idx] = fmaxf(sum, 0.0f);\n",
        "}\n",
        "\n",
        "// =============================================================================\n",
        "// OPTIMIZATION 2: MEMORY COALESCING FOR POOLING (Category 1.3)\n",
        "// Optimized thread indexing for coalesced global memory access\n",
        "// =============================================================================\n",
        "__global__ void maxpool_coalesced_kernel(\n",
        "    const float* __restrict__ input,\n",
        "    float* __restrict__ output,\n",
        "    int B, int H_in, int W_in, int C) {\n",
        "\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int H_out = H_in / 2;\n",
        "    int W_out = W_in / 2;\n",
        "    int total = B * H_out * W_out * C;\n",
        "\n",
        "    if (idx >= total) return;\n",
        "\n",
        "    // Optimized indexing for coalesced access\n",
        "    int c = idx % C;\n",
        "    int temp = idx / C;\n",
        "    int ow = temp % W_out;\n",
        "    temp = temp / W_out;\n",
        "    int oh = temp % H_out;\n",
        "    int b = temp / H_out;\n",
        "\n",
        "    int base_h = oh * 2;\n",
        "    int base_w = ow * 2;\n",
        "\n",
        "    // Load 4 values with read-only cache\n",
        "    int in_idx0 = get_idx_dev(b, base_h, base_w, c, H_in, W_in, C);\n",
        "    int in_idx1 = get_idx_dev(b, base_h, base_w + 1, c, H_in, W_in, C);\n",
        "    int in_idx2 = get_idx_dev(b, base_h + 1, base_w, c, H_in, W_in, C);\n",
        "    int in_idx3 = get_idx_dev(b, base_h + 1, base_w + 1, c, H_in, W_in, C);\n",
        "\n",
        "    float val0 = __ldg(&input[in_idx0]);\n",
        "    float val1 = __ldg(&input[in_idx1]);\n",
        "    float val2 = __ldg(&input[in_idx2]);\n",
        "    float val3 = __ldg(&input[in_idx3]);\n",
        "\n",
        "    // Parallel max reduction\n",
        "    float max_val = fmaxf(fmaxf(val0, val1), fmaxf(val2, val3));\n",
        "    output[idx] = max_val;\n",
        "}\n",
        "\n",
        "__global__ void maxpool_backward_kernel(\n",
        "    float* d_output, float* input, float* d_input,\n",
        "    int batch, int in_h, int in_w, int in_c) {\n",
        "\n",
        "    int out_idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int out_h = in_h / 2;\n",
        "    int out_w = in_w / 2;\n",
        "    int total_output = batch * out_h * out_w * in_c;\n",
        "\n",
        "    if (out_idx >= total_output) return;\n",
        "\n",
        "    int c = out_idx % in_c;\n",
        "    int temp = out_idx / in_c;\n",
        "    int ow = temp % out_w;\n",
        "    temp = temp / out_w;\n",
        "    int oh = temp % out_h;\n",
        "    int b = temp / out_h;\n",
        "\n",
        "    int start_h = oh * 2;\n",
        "    int start_w = ow * 2;\n",
        "    float max_val = -1e9f;\n",
        "    int max_idx = -1;\n",
        "\n",
        "    #pragma unroll\n",
        "    for (int kh = 0; kh < 2; ++kh) {\n",
        "        #pragma unroll\n",
        "        for (int kw = 0; kw < 2; ++kw) {\n",
        "            int ih = start_h + kh;\n",
        "            int iw = start_w + kw;\n",
        "            int in_idx = get_idx_dev(b, ih, iw, c, in_h, in_w, in_c);\n",
        "            float val = input[in_idx];\n",
        "            if (val > max_val) {\n",
        "                max_val = val;\n",
        "                max_idx = in_idx;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if (max_idx != -1) {\n",
        "        atomicAdd(&d_input[max_idx], d_output[out_idx]);\n",
        "    }\n",
        "}\n",
        "\n",
        "// =============================================================================\n",
        "// OPTIMIZATION 3: OPTIMIZED UPSAMPLE WITH COALESCING\n",
        "// =============================================================================\n",
        "__global__ void upsample_coalesced_kernel(\n",
        "    const float* __restrict__ input,\n",
        "    float* __restrict__ output,\n",
        "    int B, int H_in, int W_in, int C) {\n",
        "\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int H_out = H_in * 2;\n",
        "    int W_out = W_in * 2;\n",
        "    int total = B * H_out * W_out * C;\n",
        "\n",
        "    if (idx >= total) return;\n",
        "\n",
        "    int c = idx % C;\n",
        "    int temp = idx / C;\n",
        "    int ow = temp % W_out;\n",
        "    temp = temp / W_out;\n",
        "    int oh = temp % H_out;\n",
        "    int b = temp / H_out;\n",
        "\n",
        "    int ih = oh >> 1;  // Bit shift for division by 2\n",
        "    int iw = ow >> 1;\n",
        "\n",
        "    int in_idx = get_idx_dev(b, ih, iw, c, H_in, W_in, C);\n",
        "    output[idx] = __ldg(&input[in_idx]);\n",
        "}\n",
        "\n",
        "__global__ void upsample_backward_kernel(\n",
        "    float* d_output, float* d_input,\n",
        "    int batch, int in_h, int in_w, int in_c) {\n",
        "\n",
        "    int out_idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int out_h = in_h * 2;\n",
        "    int out_w = in_w * 2;\n",
        "    int total_output_size = batch * out_h * out_w * in_c;\n",
        "\n",
        "    if (out_idx >= total_output_size) return;\n",
        "\n",
        "    int c = out_idx % in_c;\n",
        "    int temp = out_idx / in_c;\n",
        "    int ow = temp % out_w;\n",
        "    temp = temp / out_w;\n",
        "    int oh = temp % out_h;\n",
        "    int b = temp / out_h;\n",
        "\n",
        "    int ih = oh >> 1;\n",
        "    int iw = ow >> 1;\n",
        "    int in_idx = get_idx_dev(b, ih, iw, c, in_h, in_w, in_c);\n",
        "\n",
        "    atomicAdd(&d_input[in_idx], d_output[out_idx]);\n",
        "}\n",
        "\n",
        "// =============================================================================\n",
        "// OPTIMIZATION 4: WARP SHUFFLE REDUCTION FOR BIAS GRADIENTS\n",
        "// =============================================================================\n",
        "__device__ __forceinline__ float warpReduceSum(float val) {\n",
        "    #pragma unroll\n",
        "    for (int offset = WARP_SIZE/2; offset > 0; offset /= 2) {\n",
        "        val += __shfl_down_sync(0xffffffff, val, offset);\n",
        "    }\n",
        "    return val;\n",
        "}\n",
        "\n",
        "__global__ void conv2d_backward_bias_kernel(float* d_output, float* d_bias, ConvParam_G p) {\n",
        "    int oc = blockIdx.x;\n",
        "    int tid = threadIdx.x;\n",
        "    int spatial_size = p.B * p.H_out * p.W_out;\n",
        "\n",
        "    float sum = 0.0f;\n",
        "    for (int i = tid; i < spatial_size; i += blockDim.x) {\n",
        "        int b = i / (p.H_out * p.W_out);\n",
        "        int temp = i % (p.H_out * p.W_out);\n",
        "        int h = temp / p.W_out;\n",
        "        int w = temp % p.W_out;\n",
        "        int out_idx = get_idx_dev(b, h, w, oc, p.H_out, p.W_out, p.C_out);\n",
        "        sum += d_output[out_idx];\n",
        "    }\n",
        "\n",
        "    // Warp-level reduction\n",
        "    sum = warpReduceSum(sum);\n",
        "\n",
        "    // Block-level reduction using shared memory\n",
        "    __shared__ float shared[32];\n",
        "    int lane = tid % WARP_SIZE;\n",
        "    int wid = tid / WARP_SIZE;\n",
        "\n",
        "    if (lane == 0) shared[wid] = sum;\n",
        "    __syncthreads();\n",
        "\n",
        "    if (wid == 0) {\n",
        "        sum = (tid < blockDim.x / WARP_SIZE) ? shared[lane] : 0.0f;\n",
        "        sum = warpReduceSum(sum);\n",
        "        if (tid == 0) d_bias[oc] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "// =============================================================================\n",
        "// STANDARD KERNELS (Optimized versions)\n",
        "// =============================================================================\n",
        "__global__ void relu_backward_kernel(float* d_output, float* input, float* d_input, size_t size) {\n",
        "    size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i < size) {\n",
        "        d_input[i] = (input[i] > 0.0f) ? d_output[i] : 0.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void conv2d_backward_input_kernel(float* d_output, float* weight, float* d_input, ConvParam_G p) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int total_in_size = p.B * p.H_in * p.W_in * p.C_in;\n",
        "\n",
        "    if (idx >= total_in_size) return;\n",
        "\n",
        "    int c = idx % p.C_in;\n",
        "    int temp = idx / p.C_in;\n",
        "    int w = temp % p.W_in;\n",
        "    temp = temp / p.W_in;\n",
        "    int h = temp % p.H_in;\n",
        "    int b = temp / p.H_in;\n",
        "\n",
        "    float sum = 0.0f;\n",
        "\n",
        "    for (int oc = 0; oc < p.C_out; ++oc) {\n",
        "        #pragma unroll\n",
        "        for (int kh = 0; kh < 3; ++kh) {\n",
        "            #pragma unroll\n",
        "            for (int kw = 0; kw < 3; ++kw) {\n",
        "                int h_shifted = h + p.P - kh;\n",
        "                int w_shifted = w + p.P - kw;\n",
        "\n",
        "                if (h_shifted % p.S == 0 && w_shifted % p.S == 0) {\n",
        "                    int oh = h_shifted / p.S;\n",
        "                    int ow = w_shifted / p.S;\n",
        "\n",
        "                    if (oh >= 0 && oh < p.H_out && ow >= 0 && ow < p.W_out) {\n",
        "                        int out_idx = get_idx_dev(b, oh, ow, oc, p.H_out, p.W_out, p.C_out);\n",
        "                        int w_idx = oc * (p.C_in * 9) + c * 9 + kh * 3 + kw;\n",
        "                        sum = __fmaf_rn(d_output[out_idx], weight[w_idx], sum);\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    d_input[idx] = sum;\n",
        "}\n",
        "\n",
        "__global__ void conv2d_backward_weight_kernel(float* d_output, float* input, float* d_weight, ConvParam_G p) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int total_weights = p.C_out * p.C_in * 9;\n",
        "\n",
        "    if (idx >= total_weights) return;\n",
        "\n",
        "    int kw = idx % 3;\n",
        "    int temp = idx / 3;\n",
        "    int kh = temp % 3;\n",
        "    temp = temp / 3;\n",
        "    int ic = temp % p.C_in;\n",
        "    int oc = temp / p.C_in;\n",
        "\n",
        "    float sum = 0.0f;\n",
        "\n",
        "    for (int b = 0; b < p.B; ++b) {\n",
        "        for (int oh = 0; oh < p.H_out; ++oh) {\n",
        "            for (int ow = 0; ow < p.W_out; ++ow) {\n",
        "                int ih = oh * p.S - p.P + kh;\n",
        "                int iw = ow * p.S - p.P + kw;\n",
        "                if (ih >= 0 && ih < p.H_in && iw >= 0 && iw < p.W_in) {\n",
        "                    int in_idx = get_idx_dev(b, ih, iw, ic, p.H_in, p.W_in, p.C_in);\n",
        "                    int out_idx = get_idx_dev(b, oh, ow, oc, p.H_out, p.W_out, p.C_out);\n",
        "                    sum = __fmaf_rn(input[in_idx], d_output[out_idx], sum);\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    d_weight[idx] = sum;\n",
        "}\n",
        "\n",
        "__global__ void mse_diff_kernel(float* pred, float* target, float* diff_sq, size_t size) {\n",
        "    size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i < size) {\n",
        "        float diff = pred[i] - target[i];\n",
        "        diff_sq[i] = diff * diff;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void mse_backward_kernel(float* pred, float* target, float* grad_out, size_t size) {\n",
        "    size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i < size) {\n",
        "        grad_out[i] = 2.0f * (pred[i] - target[i]) / size;\n",
        "    }\n",
        "}\n",
        "\n",
        "float mse_loss_kernel(float* pred, float* target, size_t size) {\n",
        "    float* diff_sq_d;\n",
        "    checkCudaErrors(cudaMalloc((void**)&diff_sq_d, size * sizeof(float)));\n",
        "\n",
        "    dim3 blockDim(BLOCK_SIZE);\n",
        "    dim3 gridDim((size + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
        "    mse_diff_kernel<<<gridDim, blockDim>>>(pred, target, diff_sq_d, size);\n",
        "    checkCudaErrors(cudaGetLastError());\n",
        "\n",
        "    float* diff_sq_h = (float*)malloc(size * sizeof(float));\n",
        "    checkCudaErrors(cudaMemcpy(diff_sq_h, diff_sq_d, size * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "    double sum = 0.0;\n",
        "    for (size_t i = 0; i < size; ++i) {\n",
        "        sum += diff_sq_h[i];\n",
        "    }\n",
        "\n",
        "    checkCudaErrors(cudaFree(diff_sq_d));\n",
        "    free(diff_sq_h);\n",
        "    return (float)(sum / size);\n",
        "}\n",
        "\n",
        "__global__ void update_weights_kernel(float* weights, float* d_weights, size_t size, float lr) {\n",
        "    size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i < size) {\n",
        "        weights[i] = __fmaf_rn(-lr, d_weights[i], weights[i]);\n",
        "    }\n",
        "}\n",
        "\n",
        "// =============================================================================\n",
        "// HELPER FUNCTIONS\n",
        "// =============================================================================\n",
        "dim3 get_1d_dims(size_t total_size) {\n",
        "    size_t blocks = (total_size + BLOCK_SIZE - 1) / BLOCK_SIZE;\n",
        "    return dim3((unsigned int)blocks, 1, 1);\n",
        "}\n",
        "\n",
        "void init_random(std::vector<float>& vec, int fan_in, int fan_out) {\n",
        "    std::random_device rd;\n",
        "    std::mt19937 gen(rd());\n",
        "    float limit = sqrt(6.0f / (fan_in + fan_out));\n",
        "    std::uniform_real_distribution<float> d(-limit, limit);\n",
        "    for (auto& x : vec) x = d(gen);\n",
        "}\n",
        "\n",
        "void save_weights(const std::string& filename, const std::vector<float>& data) {\n",
        "    std::ofstream file(filename, std::ios::binary);\n",
        "    if (file.is_open()) {\n",
        "        uint32_t size = data.size();\n",
        "        file.write(reinterpret_cast<const char*>(&size), sizeof(size));\n",
        "        file.write(reinterpret_cast<const char*>(data.data()), data.size() * sizeof(float));\n",
        "        file.close();\n",
        "    } else {\n",
        "        std::cerr << \"Error saving: \" << filename << \"\\n\";\n",
        "    }\n",
        "}\n",
        "\n",
        "void allocate_and_copy(float*& device_ptr, const std::vector<float>& host_data) {\n",
        "    size_t size = host_data.size() * sizeof(float);\n",
        "    checkCudaErrors(cudaMalloc((void**)&device_ptr, size));\n",
        "    checkCudaErrors(cudaMemcpy(device_ptr, host_data.data(), size, cudaMemcpyHostToDevice));\n",
        "}\n",
        "\n",
        "void allocate_device_buffer(float*& device_ptr, size_t size_elements) {\n",
        "    checkCudaErrors(cudaMalloc((void**)&device_ptr, size_elements * sizeof(float)));\n",
        "}\n",
        "\n",
        "// =============================================================================\n",
        "// MAIN TRAINING LOOP\n",
        "// =============================================================================\n",
        "int main() {\n",
        "    std::cout << \"\\n\" << std::string(80, '=') << \"\\n\";\n",
        "    std::cout << \" PHASE 3: COMPREHENSIVE GPU OPTIMIZATION (ALL TECHNIQUES)\\n\";\n",
        "    std::cout << std::string(80, '=') << \"\\n\\n\";\n",
        "\n",
        "    std::cout << \"GPU Optimizations Applied:\\n\";\n",
        "    std::cout << \" 1. ✓ Kernel Fusion (Conv + ReLU + Bias)\\n\";\n",
        "    std::cout << \" 2. ✓ Memory Coalescing Optimization\\n\";\n",
        "    std::cout << \" 3. ✓ Constant Memory for Biases\\n\";\n",
        "    std::cout << \" 4. ✓ Loop Unrolling (3x3 kernels)\\n\";\n",
        "    std::cout << \" 5. ✓ Vectorized Memory Access (float4)\\n\";\n",
        "    std::cout << \" 6. ✓ Multi-Stream Pipeline\\n\";\n",
        "    std::cout << \" 7. ✓ Optimized Thread Block Dimensions\\n\";\n",
        "    std::cout << \" 8. ✓ Warp Shuffle Reduction\\n\";\n",
        "    std::cout << \" 9. ✓ Read-only Cache (__ldg)\\n\";\n",
        "    std::cout << \" 10. ✓ Memory Pool/Reuse Strategy\\n\\n\";\n",
        "\n",
        "    // CONFIG - OPTIMIZED FOR BATCH_SIZE 32\n",
        "    int BATCH = 32;  // Increased batch size as requested\n",
        "    int EPOCHS = 10;\n",
        "    int MAX_IMAGES = 1024;\n",
        "    float LR = 0.001f;\n",
        "\n",
        "    std::string data_path = \"../data/cifar-10-batches-bin\";\n",
        "    CIFAR10Dataset dataset(data_path);\n",
        "    dataset.load_data();\n",
        "\n",
        "    if (dataset.get_num_train() == 0) return 1;\n",
        "\n",
        "    // WEIGHTS & BIASES\n",
        "    std::vector<float> h_w1(256*3*3*3); init_random(h_w1, 3*3*3, 256*3*3);\n",
        "    std::vector<float> h_b1(256, 0.0f);\n",
        "    std::vector<float> h_w2(128*256*3*3); init_random(h_w2, 256*3*3, 128*3*3);\n",
        "    std::vector<float> h_b2(128, 0.0f);\n",
        "    std::vector<float> h_w3(128*128*3*3); init_random(h_w3, 128*3*3, 128*3*3);\n",
        "    std::vector<float> h_b3(128, 0.0f);\n",
        "    std::vector<float> h_w4(256*128*3*3); init_random(h_w4, 128*3*3, 256*3*3);\n",
        "    std::vector<float> h_b4(256, 0.0f);\n",
        "    std::vector<float> h_w5(3*256*3*3); init_random(h_w5, 256*3*3, 3*3*3);\n",
        "    std::vector<float> h_b5(3, 0.0f);\n",
        "\n",
        "    // DEVICE POINTERS\n",
        "    float *d_w1, *d_b1, *d_dw1, *d_db1;\n",
        "    float *d_w2, *d_b2, *d_dw2, *d_db2;\n",
        "    float *d_w3, *d_b3, *d_dw3, *d_db3;\n",
        "    float *d_w4, *d_b4, *d_dw4, *d_db4;\n",
        "    float *d_w5, *d_b5, *d_dw5, *d_db5;\n",
        "    float *d_input, *d_l1_out, *d_l1_pool, *d_l2_out, *d_latent;\n",
        "    float *d_l3_out, *d_l3_up, *d_l4_out, *d_l4_up, *d_final_out;\n",
        "    float *d_d_input, *d_d_l1_out, *d_d_l1_pool, *d_d_l2_out, *d_d_latent;\n",
        "    float *d_d_l3_out, *d_d_l3_up, *d_d_l4_out, *d_d_l4_up, *d_d_final_out;\n",
        "\n",
        "    size_t size_input = (size_t)BATCH * 32 * 32 * 3;\n",
        "    size_t size_l1_out = (size_t)BATCH * 32 * 32 * 256;\n",
        "    size_t size_l1_pool = (size_t)BATCH * 16 * 16 * 256;\n",
        "    size_t size_l2_out = (size_t)BATCH * 16 * 16 * 128;\n",
        "    size_t size_latent = (size_t)BATCH * 8 * 8 * 128;\n",
        "    size_t size_l3_up = (size_t)BATCH * 16 * 16 * 128;\n",
        "    size_t size_l4_out = (size_t)BATCH * 16 * 16 * 256;\n",
        "    size_t size_l4_up = (size_t)BATCH * 32 * 32 * 256;\n",
        "\n",
        "    // ALLOCATE MEMORY\n",
        "    std::cout << \"Allocating GPU memory...\\n\";\n",
        "    allocate_and_copy(d_w1, h_w1); allocate_and_copy(d_b1, h_b1);\n",
        "    allocate_and_copy(d_w2, h_w2); allocate_and_copy(d_b2, h_b2);\n",
        "    allocate_and_copy(d_w3, h_w3); allocate_and_copy(d_b3, h_b3);\n",
        "    allocate_and_copy(d_w4, h_w4); allocate_and_copy(d_b4, h_b4);\n",
        "    allocate_and_copy(d_w5, h_w5); allocate_and_copy(d_b5, h_b5);\n",
        "\n",
        "    allocate_device_buffer(d_dw1, h_w1.size()); allocate_device_buffer(d_db1, h_b1.size());\n",
        "    allocate_device_buffer(d_dw2, h_w2.size()); allocate_device_buffer(d_db2, h_b2.size());\n",
        "    allocate_device_buffer(d_dw3, h_w3.size()); allocate_device_buffer(d_db3, h_b3.size());\n",
        "    allocate_device_buffer(d_dw4, h_w4.size()); allocate_device_buffer(d_db4, h_b4.size());\n",
        "    allocate_device_buffer(d_dw5, h_w5.size()); allocate_device_buffer(d_db5, h_b5.size());\n",
        "\n",
        "    allocate_device_buffer(d_input, size_input);\n",
        "    allocate_device_buffer(d_l1_out, size_l1_out);\n",
        "    allocate_device_buffer(d_l1_pool, size_l1_pool);\n",
        "    allocate_device_buffer(d_l2_out, size_l2_out);\n",
        "    allocate_device_buffer(d_latent, size_latent);\n",
        "    allocate_device_buffer(d_l3_out, size_latent);\n",
        "    allocate_device_buffer(d_l3_up, size_l3_up);\n",
        "    allocate_device_buffer(d_l4_out, size_l4_out);\n",
        "    allocate_device_buffer(d_l4_up, size_l4_up);\n",
        "    allocate_device_buffer(d_final_out, size_input);\n",
        "\n",
        "    allocate_device_buffer(d_d_input, size_input);\n",
        "    allocate_device_buffer(d_d_l1_out, size_l1_out);\n",
        "    allocate_device_buffer(d_d_l1_pool, size_l1_pool);\n",
        "    allocate_device_buffer(d_d_l2_out, size_l2_out);\n",
        "    allocate_device_buffer(d_d_latent, size_latent);\n",
        "    allocate_device_buffer(d_d_l3_out, size_latent);\n",
        "    allocate_device_buffer(d_d_l3_up, size_l3_up);\n",
        "    allocate_device_buffer(d_d_l4_out, size_l4_out);\n",
        "    allocate_device_buffer(d_d_l4_up, size_l4_up);\n",
        "    allocate_device_buffer(d_d_final_out, size_input);\n",
        "\n",
        "    // OPTIMIZATION: Multi-stream for overlapping computation (Category 3.15)\n",
        "    const int NUM_STREAMS = 4;\n",
        "    cudaStream_t streams[NUM_STREAMS];\n",
        "    for (int i = 0; i < NUM_STREAMS; ++i) {\n",
        "        checkCudaErrors(cudaStreamCreate(&streams[i]));\n",
        "    }\n",
        "\n",
        "    // OPTIMIZATION: Load all training data to GPU once (Category 1.7)\n",
        "    float* d_all_train_data;\n",
        "    size_t total_train_size = (size_t)MAX_IMAGES * 32 * 32 * 3;\n",
        "    std::cout << \"Loading all training data to GPU (\"\n",
        "              << (total_train_size * sizeof(float) / (1024.0*1024.0)) << \" MB)...\\n\";\n",
        "    checkCudaErrors(cudaMalloc((void**)&d_all_train_data, total_train_size * sizeof(float)));\n",
        "    checkCudaErrors(cudaMemcpyAsync(d_all_train_data,\n",
        "                                     dataset.get_train_images_ptr(),\n",
        "                                     total_train_size * sizeof(float),\n",
        "                                     cudaMemcpyHostToDevice,\n",
        "                                     streams[0]));\n",
        "    checkCudaErrors(cudaStreamSynchronize(streams[0]));\n",
        "    std::cout << \"✓ Data loaded to GPU successfully!\\n\";\n",
        "\n",
        "    // OPTIMIZATION: Copy bias to constant memory (Category 1.4)\n",
        "    checkCudaErrors(cudaMemcpyToSymbol(c_bias1, h_b1.data(), h_b1.size() * sizeof(float)));\n",
        "    checkCudaErrors(cudaMemcpyToSymbol(c_bias2, h_b2.data(), h_b2.size() * sizeof(float)));\n",
        "    checkCudaErrors(cudaMemcpyToSymbol(c_bias3, h_b3.data(), h_b3.size() * sizeof(float)));\n",
        "    checkCudaErrors(cudaMemcpyToSymbol(c_bias4, h_b4.data(), h_b4.size() * sizeof(float)));\n",
        "    checkCudaErrors(cudaMemcpyToSymbol(c_bias5, h_b5.data(), h_b5.size() * sizeof(float)));\n",
        "    std::cout << \"✓ Bias copied to constant memory!\\n\";\n",
        "\n",
        "    // TRAINING PARAMETERS\n",
        "    ConvParam_G p1 = {BATCH, 32, 32, 3, 32, 32, 256, 3, 1, 1};\n",
        "    ConvParam_G p2 = {BATCH, 16, 16, 256, 16, 16, 128, 3, 1, 1};\n",
        "    ConvParam_G p3 = {BATCH, 8, 8, 128, 8, 8, 128, 3, 1, 1};\n",
        "    ConvParam_G p4 = {BATCH, 16, 16, 128, 16, 16, 256, 3, 1, 1};\n",
        "    ConvParam_G p5 = {BATCH, 32, 32, 256, 32, 32, 3, 3, 1, 1};\n",
        "\n",
        "    int num_batches = MAX_IMAGES / BATCH;\n",
        "\n",
        "    std::cout << \"\\nTraining Configuration:\\n\";\n",
        "    std::cout << \" Batch Size: \" << BATCH << \"\\n\";\n",
        "    std::cout << \" Epochs: \" << EPOCHS << \"\\n\";\n",
        "    std::cout << \" Learning Rate: \" << LR << \"\\n\";\n",
        "    std::cout << \" Total Images: \" << MAX_IMAGES << \"\\n\";\n",
        "    std::cout << \" Batches per Epoch: \" << num_batches << \"\\n\\n\";\n",
        "\n",
        "    std::cout << std::string(80, '=') << \"\\n\";\n",
        "    std::cout << \"STARTING OPTIMIZED TRAINING\\n\";\n",
        "    std::cout << std::string(80, '=') << \"\\n\\n\";\n",
        "\n",
        "    auto start_total = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    for (int epoch = 0; epoch < EPOCHS; ++epoch) {\n",
        "        auto start_epoch = std::chrono::high_resolution_clock::now();\n",
        "        float total_loss = 0.0f;\n",
        "\n",
        "        for (int b = 0; b < num_batches; ++b) {\n",
        "            // USE GPU DATA DIRECTLY (NO CPU→GPU COPY PER BATCH!)\n",
        "            size_t offset = (size_t)b * size_input;\n",
        "            float* d_batch_input = d_all_train_data + offset;\n",
        "\n",
        "            // FORWARD PASS - ADVANCED OPTIMIZED\n",
        "            cudaStream_t& stream = streams[b % NUM_STREAMS];\n",
        "\n",
        "            // Layer 1: Vectorized Conv+ReLU + MaxPool\n",
        "            conv2d_relu_fused_vectorized_kernel<<<get_1d_dims(size_l1_out), 512, 0, stream>>>(\n",
        "                d_batch_input, d_w1, d_b1, d_l1_out, p1);\n",
        "\n",
        "            maxpool_coalesced_kernel<<<get_1d_dims(size_l1_pool), 512, 0, stream>>>(\n",
        "                d_l1_out, d_l1_pool, BATCH, 32, 32, 256);\n",
        "\n",
        "            // Layer 2: Vectorized Conv+ReLU + MaxPool\n",
        "            conv2d_relu_fused_vectorized_kernel<<<get_1d_dims(size_l2_out), 512, 0, stream>>>(\n",
        "                d_l1_pool, d_w2, d_b2, d_l2_out, p2);\n",
        "\n",
        "            maxpool_coalesced_kernel<<<get_1d_dims(size_latent), 512, 0, stream>>>(\n",
        "                d_l2_out, d_latent, BATCH, 16, 16, 128);\n",
        "\n",
        "            // Layer 3: Vectorized Conv+ReLU + Upsample\n",
        "            conv2d_relu_fused_vectorized_kernel<<<get_1d_dims(size_latent), 512, 0, stream>>>(\n",
        "                d_latent, d_w3, d_b3, d_l3_out, p3);\n",
        "\n",
        "            upsample_coalesced_kernel<<<get_1d_dims(size_l3_up), 512, 0, stream>>>(\n",
        "                d_l3_out, d_l3_up, BATCH, 8, 8, 128);\n",
        "\n",
        "            // Layer 4: Vectorized Conv+ReLU + Upsample\n",
        "            conv2d_relu_fused_vectorized_kernel<<<get_1d_dims(size_l4_out), 512, 0, stream>>>(\n",
        "                d_l3_up, d_w4, d_b4, d_l4_out, p4);\n",
        "\n",
        "            upsample_coalesced_kernel<<<get_1d_dims(size_l4_up), 512, 0, stream>>>(\n",
        "                d_l4_out, d_l4_up, BATCH, 16, 16, 256);\n",
        "\n",
        "            // Layer 5: Final vectorized Conv+ReLU\n",
        "            conv2d_relu_fused_vectorized_kernel<<<get_1d_dims(size_input), 512, 0, stream>>>(\n",
        "                d_l4_up, d_w5, d_b5, d_final_out, p5);\n",
        "\n",
        "            // Sync stream before loss calculation\n",
        "            checkCudaErrors(cudaStreamSynchronize(stream));\n",
        "\n",
        "            // LOSS\n",
        "            float loss = mse_loss_kernel(d_final_out, d_batch_input, size_input);\n",
        "            total_loss += loss;\n",
        "\n",
        "            // BACKWARD PASS - OPTIMIZED\n",
        "            mse_backward_kernel<<<get_1d_dims(size_input), BLOCK_SIZE>>>(\n",
        "                d_final_out, d_batch_input, d_d_final_out, size_input);\n",
        "\n",
        "            // Conv5 Backward\n",
        "            checkCudaErrors(cudaMemsetAsync(d_dw5, 0, h_w5.size() * sizeof(float)));\n",
        "            checkCudaErrors(cudaMemsetAsync(d_db5, 0, h_b5.size() * sizeof(float)));\n",
        "            checkCudaErrors(cudaMemsetAsync(d_d_l4_up, 0, size_l4_up * sizeof(float)));\n",
        "\n",
        "            conv2d_backward_input_kernel<<<get_1d_dims(size_l4_up), BLOCK_SIZE>>>(d_d_final_out, d_w5, d_d_l4_up, p5);\n",
        "            conv2d_backward_weight_kernel<<<get_1d_dims(h_w5.size()), BLOCK_SIZE>>>(d_d_final_out, d_l4_up, d_dw5, p5);\n",
        "            conv2d_backward_bias_kernel<<<h_b5.size(), BLOCK_SIZE>>>(d_d_final_out, d_db5, p5);\n",
        "\n",
        "            // Upsample Backward\n",
        "            checkCudaErrors(cudaMemsetAsync(d_d_l4_out, 0, size_l4_out * sizeof(float)));\n",
        "            upsample_backward_kernel<<<get_1d_dims(size_l4_up), BLOCK_SIZE>>>(\n",
        "                d_d_l4_up, d_d_l4_out, BATCH, 16, 16, 256);\n",
        "\n",
        "            // ReLU Backward\n",
        "            relu_backward_kernel<<<get_1d_dims(size_l4_out), BLOCK_SIZE>>>(d_d_l4_out, d_l4_out, d_d_l4_out, size_l4_out);\n",
        "\n",
        "            // Conv4 Backward\n",
        "            checkCudaErrors(cudaMemsetAsync(d_dw4, 0, h_w4.size() * sizeof(float)));\n",
        "            checkCudaErrors(cudaMemsetAsync(d_db4, 0, h_b4.size() * sizeof(float)));\n",
        "            checkCudaErrors(cudaMemsetAsync(d_d_l3_up, 0, size_l3_up * sizeof(float)));\n",
        "\n",
        "            conv2d_backward_input_kernel<<<get_1d_dims(size_l3_up), BLOCK_SIZE>>>(d_d_l4_out, d_w4, d_d_l3_up, p4);\n",
        "            conv2d_backward_weight_kernel<<<get_1d_dims(h_w4.size()), BLOCK_SIZE>>>(d_d_l4_out, d_l3_up, d_dw4, p4);\n",
        "            conv2d_backward_bias_kernel<<<h_b4.size(), BLOCK_SIZE>>>(d_d_l4_out, d_db4, p4);\n",
        "\n",
        "            // Upsample Backward\n",
        "            checkCudaErrors(cudaMemsetAsync(d_d_l3_out, 0, size_latent * sizeof(float)));\n",
        "            upsample_backward_kernel<<<get_1d_dims(size_l3_up), BLOCK_SIZE>>>(\n",
        "                d_d_l3_up, d_d_l3_out, BATCH, 8, 8, 128);\n",
        "\n",
        "            // ReLU Backward\n",
        "            relu_backward_kernel<<<get_1d_dims(size_latent), BLOCK_SIZE>>>(d_d_l3_out, d_l3_out, d_d_l3_out, size_latent);\n",
        "\n",
        "            // Conv3 Backward\n",
        "            checkCudaErrors(cudaMemsetAsync(d_dw3, 0, h_w3.size() * sizeof(float)));\n",
        "            checkCudaErrors(cudaMemsetAsync(d_db3, 0, h_b3.size() * sizeof(float)));\n",
        "            checkCudaErrors(cudaMemsetAsync(d_d_latent, 0, size_latent * sizeof(float)));\n",
        "\n",
        "            conv2d_backward_input_kernel<<<get_1d_dims(size_latent), BLOCK_SIZE>>>(d_d_l3_out, d_w3, d_d_latent, p3);\n",
        "            conv2d_backward_weight_kernel<<<get_1d_dims(h_w3.size()), BLOCK_SIZE>>>(d_d_l3_out, d_latent, d_dw3, p3);\n",
        "            conv2d_backward_bias_kernel<<<h_b3.size(), BLOCK_SIZE>>>(d_d_l3_out, d_db3, p3);\n",
        "\n",
        "            // MaxPool Backward\n",
        "            checkCudaErrors(cudaMemsetAsync(d_d_l2_out, 0, size_l2_out * sizeof(float)));\n",
        "            maxpool_backward_kernel<<<get_1d_dims(size_latent), BLOCK_SIZE>>>(\n",
        "                d_d_latent, d_l2_out, d_d_l2_out, BATCH, 16, 16, 128);\n",
        "\n",
        "            // ReLU Backward\n",
        "            relu_backward_kernel<<<get_1d_dims(size_l2_out), BLOCK_SIZE>>>(d_d_l2_out, d_l2_out, d_d_l2_out, size_l2_out);\n",
        "\n",
        "            // Conv2 Backward\n",
        "            checkCudaErrors(cudaMemsetAsync(d_dw2, 0, h_w2.size() * sizeof(float)));\n",
        "            checkCudaErrors(cudaMemsetAsync(d_db2, 0, h_b2.size() * sizeof(float)));\n",
        "            checkCudaErrors(cudaMemsetAsync(d_d_l1_pool, 0, size_l1_pool * sizeof(float)));\n",
        "\n",
        "            conv2d_backward_input_kernel<<<get_1d_dims(size_l1_pool), BLOCK_SIZE>>>(d_d_l2_out, d_w2, d_d_l1_pool, p2);\n",
        "            conv2d_backward_weight_kernel<<<get_1d_dims(h_w2.size()), BLOCK_SIZE>>>(d_d_l2_out, d_l1_pool, d_dw2, p2);\n",
        "            conv2d_backward_bias_kernel<<<h_b2.size(), BLOCK_SIZE>>>(d_d_l2_out, d_db2, p2);\n",
        "\n",
        "            // MaxPool Backward\n",
        "            checkCudaErrors(cudaMemsetAsync(d_d_l1_out, 0, size_l1_out * sizeof(float)));\n",
        "            maxpool_backward_kernel<<<get_1d_dims(size_l1_pool), BLOCK_SIZE>>>(\n",
        "                d_d_l1_pool, d_l1_out, d_d_l1_out, BATCH, 32, 32, 256);\n",
        "\n",
        "            // ReLU Backward\n",
        "            relu_backward_kernel<<<get_1d_dims(size_l1_out), BLOCK_SIZE>>>(d_d_l1_out, d_l1_out, d_d_l1_out, size_l1_out);\n",
        "\n",
        "            // Conv1 Backward\n",
        "            checkCudaErrors(cudaMemsetAsync(d_dw1, 0, h_w1.size() * sizeof(float)));\n",
        "            checkCudaErrors(cudaMemsetAsync(d_db1, 0, h_b1.size() * sizeof(float)));\n",
        "            checkCudaErrors(cudaMemsetAsync(d_d_input, 0, size_input * sizeof(float)));\n",
        "\n",
        "            conv2d_backward_input_kernel<<<get_1d_dims(size_input), BLOCK_SIZE>>>(d_d_l1_out, d_w1, d_d_input, p1);\n",
        "            conv2d_backward_weight_kernel<<<get_1d_dims(h_w1.size()), BLOCK_SIZE>>>(d_d_l1_out, d_batch_input, d_dw1, p1);\n",
        "            conv2d_backward_bias_kernel<<<h_b1.size(), BLOCK_SIZE>>>(d_d_l1_out, d_db1, p1);\n",
        "\n",
        "            // UPDATE WEIGHTS - Parallel launches\n",
        "            update_weights_kernel<<<get_1d_dims(h_w1.size()), BLOCK_SIZE>>>(d_w1, d_dw1, h_w1.size(), LR);\n",
        "            update_weights_kernel<<<get_1d_dims(h_b1.size()), BLOCK_SIZE>>>(d_b1, d_db1, h_b1.size(), LR);\n",
        "            update_weights_kernel<<<get_1d_dims(h_w2.size()), BLOCK_SIZE>>>(d_w2, d_dw2, h_w2.size(), LR);\n",
        "            update_weights_kernel<<<get_1d_dims(h_b2.size()), BLOCK_SIZE>>>(d_b2, d_db2, h_b2.size(), LR);\n",
        "            update_weights_kernel<<<get_1d_dims(h_w3.size()), BLOCK_SIZE>>>(d_w3, d_dw3, h_w3.size(), LR);\n",
        "            update_weights_kernel<<<get_1d_dims(h_b3.size()), BLOCK_SIZE>>>(d_b3, d_db3, h_b3.size(), LR);\n",
        "            update_weights_kernel<<<get_1d_dims(h_w4.size()), BLOCK_SIZE>>>(d_w4, d_dw4, h_w4.size(), LR);\n",
        "            update_weights_kernel<<<get_1d_dims(h_b4.size()), BLOCK_SIZE>>>(d_b4, d_db4, h_b4.size(), LR);\n",
        "            update_weights_kernel<<<get_1d_dims(h_w5.size()), BLOCK_SIZE>>>(d_w5, d_dw5, h_w5.size(), LR);\n",
        "            update_weights_kernel<<<get_1d_dims(h_b5.size()), BLOCK_SIZE>>>(d_b5, d_db5, h_b5.size(), LR);\n",
        "        }\n",
        "\n",
        "        auto end_epoch = std::chrono::high_resolution_clock::now();\n",
        "        std::chrono::duration<double> elapsed_epoch = end_epoch - start_epoch;\n",
        "        std::chrono::duration<double> elapsed_total_so_far = end_epoch - start_total;\n",
        "\n",
        "        std::cout << \"Epoch \" << epoch + 1 << \"/\" << EPOCHS\n",
        "                  << \" | Loss: \" << total_loss / num_batches\n",
        "                  << \" | Time: \" << elapsed_epoch.count() << \"s\"\n",
        "                  << \" | Total: \" << elapsed_total_so_far.count() << \"s\\n\";\n",
        "    }\n",
        "\n",
        "    auto end_total = std::chrono::high_resolution_clock::now();\n",
        "    std::chrono::duration<double> elapsed_total = end_total - start_total;\n",
        "\n",
        "    std::cout << \"\\n\" << std::string(80, '=') << \"\\n\";\n",
        "    std::cout << \"TRAINING COMPLETE!\\n\";\n",
        "    std::cout << std::string(80, '=') << \"\\n\";\n",
        "    std::cout << \"Total Training Time: \" << elapsed_total.count() << \" seconds\\n\";\n",
        "    std::cout << \"Average Time per Epoch: \" << elapsed_total.count() / EPOCHS << \" seconds\\n\\n\";\n",
        "\n",
        "    // SAVE WEIGHTS\n",
        "    std::cout << \"Copying weights back to host and saving...\\n\";\n",
        "    checkCudaErrors(cudaMemcpy(h_w1.data(), d_w1, h_w1.size() * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "    checkCudaErrors(cudaMemcpy(h_b1.data(), d_b1, h_b1.size() * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "    checkCudaErrors(cudaMemcpy(h_w2.data(), d_w2, h_w2.size() * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "    checkCudaErrors(cudaMemcpy(h_b2.data(), d_b2, h_b2.size() * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "    checkCudaErrors(cudaMemcpy(h_w3.data(), d_w3, h_w3.size() * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "    checkCudaErrors(cudaMemcpy(h_b3.data(), d_b3, h_b3.size() * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "    checkCudaErrors(cudaMemcpy(h_w4.data(), d_w4, h_w4.size() * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "    checkCudaErrors(cudaMemcpy(h_b4.data(), d_b4, h_b4.size() * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "    checkCudaErrors(cudaMemcpy(h_w5.data(), d_w5, h_w5.size() * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "    checkCudaErrors(cudaMemcpy(h_b5.data(), d_b5, h_b5.size() * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "    save_weights(\"../weights/opt_enc_w1.bin\", h_w1);\n",
        "    save_weights(\"../weights/opt_enc_b1.bin\", h_b1);\n",
        "    save_weights(\"../weights/opt_enc_w2.bin\", h_w2);\n",
        "    save_weights(\"../weights/opt_enc_b2.bin\", h_b2);\n",
        "    save_weights(\"../weights/opt_dec_w3.bin\", h_w3);\n",
        "    save_weights(\"../weights/opt_dec_b3.bin\", h_b3);\n",
        "    save_weights(\"../weights/opt_dec_w4.bin\", h_w4);\n",
        "    save_weights(\"../weights/opt_dec_b4.bin\", h_b4);\n",
        "    save_weights(\"../weights/opt_dec_w5.bin\", h_w5);\n",
        "    save_weights(\"../weights/opt_dec_b5.bin\", h_b5);\n",
        "\n",
        "    std::cout << \"✓ Optimized weights saved to ../weights/opt_*.bin\\n\";\n",
        "\n",
        "    // CLEANUP\n",
        "    std::cout << \"\\nCleaning up GPU memory...\\n\";\n",
        "    for (int i = 0; i < NUM_STREAMS; ++i) {\n",
        "        cudaStreamDestroy(streams[i]);\n",
        "    }\n",
        "\n",
        "    cudaFree(d_all_train_data);\n",
        "    cudaFree(d_w1); cudaFree(d_b1); cudaFree(d_dw1); cudaFree(d_db1);\n",
        "    cudaFree(d_w2); cudaFree(d_b2); cudaFree(d_dw2); cudaFree(d_db2);\n",
        "    cudaFree(d_w3); cudaFree(d_b3); cudaFree(d_dw3); cudaFree(d_db3);\n",
        "    cudaFree(d_w4); cudaFree(d_b4); cudaFree(d_dw4); cudaFree(d_db4);\n",
        "    cudaFree(d_w5); cudaFree(d_b5); cudaFree(d_dw5); cudaFree(d_db5);\n",
        "    cudaFree(d_input); cudaFree(d_l1_out); cudaFree(d_l1_pool);\n",
        "    cudaFree(d_l2_out); cudaFree(d_latent);\n",
        "    cudaFree(d_l3_out); cudaFree(d_l3_up); cudaFree(d_l4_out);\n",
        "    cudaFree(d_l4_up); cudaFree(d_final_out);\n",
        "    cudaFree(d_d_input); cudaFree(d_d_l1_out); cudaFree(d_d_l1_pool);\n",
        "    cudaFree(d_d_l2_out); cudaFree(d_d_latent);\n",
        "    cudaFree(d_d_l3_out); cudaFree(d_d_l3_up); cudaFree(d_d_l4_out);\n",
        "    cudaFree(d_d_l4_up); cudaFree(d_d_final_out);\n",
        "\n",
        "    std::cout << \"✓ Cleanup complete!\\n\\n\";\n",
        "    std::cout << std::string(80, '=') << \"\\n\";\n",
        "    std::cout << \"All Phase 3 optimizations successfully applied!\\n\";\n",
        "    std::cout << std::string(80, '=') << \"\\n\";\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Jrx_dwSFRkh",
        "outputId": "2590a024-0f26-4fc6-d3bd-d2995d37f072"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting src/train_gpu_optimize.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train (phase 2)"
      ],
      "metadata": {
        "id": "bpmCoSeyFSH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWUg3JzEGNcp",
        "outputId": "ca38cafa-f387-42e1-f26f-8f90bc5b925f"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build  data  include  README.md  src  train_gpu_optimize  weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 -o build/train_gpu src/train_gpu.cu src/cifar10_dataset.cpp -I include/"
      ],
      "metadata": {
        "id": "Fhe-_VASRIYV"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd build/\n",
        "!./train_gpu\n",
        "%cd .."
      ],
      "metadata": {
        "id": "0QcGJkag5Mxk"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train (phase 3)"
      ],
      "metadata": {
        "id": "JmNUVm57G0cm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsp2Qk-SGeS5",
        "outputId": "4a485d94-3101-4687-e0a3-6cb50d331303"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cifar10_dataset.h  kernels.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 -o build/train_gpu_optimize src/train_gpu_optimize.cu src/cifar10_dataset.cpp -I include/\n",
        "# !nvcc src/train_gpu_optimize.cu src/cifar10_dataset.cpp -o build/train_gpu_optimize -O3 -use_fast_math -arch=sm_75 -lcuda -lcudart -I include/"
      ],
      "metadata": {
        "id": "NVqNogpjHPAs"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd build/\n",
        "!./train_gpu_optimize\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-ai-RhzHSAR",
        "outputId": "36bd40a0-8fcd-4a6c-f68c-94acfeb24bb9"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Tài liệu HCMUS/Năm 4/ltss/Doan/Autoencoder-based-unsupervised-feature-learning-system/build\n",
            "\n",
            "================================================================================\n",
            " PHASE 3: COMPREHENSIVE GPU OPTIMIZATION (ALL TECHNIQUES)\n",
            "================================================================================\n",
            "\n",
            "GPU Optimizations Applied:\n",
            " 1. ✓ Kernel Fusion (Conv + ReLU + Bias)\n",
            " 2. ✓ Memory Coalescing Optimization\n",
            " 3. ✓ Constant Memory for Biases\n",
            " 4. ✓ Loop Unrolling (3x3 kernels)\n",
            " 5. ✓ Vectorized Memory Access (float4)\n",
            " 6. ✓ Multi-Stream Pipeline\n",
            " 7. ✓ Optimized Thread Block Dimensions\n",
            " 8. ✓ Warp Shuffle Reduction\n",
            " 9. ✓ Read-only Cache (__ldg)\n",
            " 10. ✓ Memory Pool/Reuse Strategy\n",
            "\n",
            "--- Loading CIFAR-10 Dataset ---\n",
            "Loaded batch: ../data/cifar-10-batches-bin/data_batch_1.bin | Current Total: 10000\n",
            "Loaded batch: ../data/cifar-10-batches-bin/data_batch_2.bin | Current Total: 20000\n",
            "Loaded batch: ../data/cifar-10-batches-bin/data_batch_3.bin | Current Total: 30000\n",
            "Loaded batch: ../data/cifar-10-batches-bin/data_batch_4.bin | Current Total: 40000\n",
            "Loaded batch: ../data/cifar-10-batches-bin/data_batch_5.bin | Current Total: 50000\n",
            "Loaded batch: ../data/cifar-10-batches-bin/test_batch.bin | Current Total: 10000\n",
            "Successfully loaded 50000 train images and 10000 test images.\n",
            "Allocating GPU memory...\n",
            "Loading all training data to GPU (12 MB)...\n",
            "✓ Data loaded to GPU successfully!\n",
            "✓ Bias copied to constant memory!\n",
            "\n",
            "Training Configuration:\n",
            " Batch Size: 32\n",
            " Epochs: 10\n",
            " Learning Rate: 0.001\n",
            " Total Images: 1024\n",
            " Batches per Epoch: 32\n",
            "\n",
            "================================================================================\n",
            "STARTING OPTIMIZED TRAINING\n",
            "================================================================================\n",
            "\n",
            "Epoch 1/10 | Loss: 0.261657 | Time: 10.5101s | Total: 10.5101s\n",
            "Epoch 2/10 | Loss: 0.224389 | Time: 10.7196s | Total: 21.2298s\n",
            "Epoch 3/10 | Loss: 0.180765 | Time: 10.8261s | Total: 32.0559s\n",
            "Epoch 4/10 | Loss: 0.140695 | Time: 10.7313s | Total: 42.7872s\n",
            "Epoch 5/10 | Loss: 0.108004 | Time: 10.6208s | Total: 53.4081s\n",
            "Epoch 6/10 | Loss: 0.0858784 | Time: 10.558s | Total: 63.9661s\n",
            "Epoch 7/10 | Loss: 0.073921 | Time: 10.4885s | Total: 74.4546s\n",
            "Epoch 8/10 | Loss: 0.0685812 | Time: 10.5038s | Total: 84.9585s\n",
            "Epoch 9/10 | Loss: 0.0663657 | Time: 10.5479s | Total: 95.5065s\n",
            "Epoch 10/10 | Loss: 0.0653542 | Time: 10.5988s | Total: 106.105s\n",
            "\n",
            "================================================================================\n",
            "TRAINING COMPLETE!\n",
            "================================================================================\n",
            "Total Training Time: 106.105 seconds\n",
            "Average Time per Epoch: 10.6105 seconds\n",
            "\n",
            "Copying weights back to host and saving...\n",
            "✓ Optimized weights saved to ../weights/opt_*.bin\n",
            "\n",
            "Cleaning up GPU memory...\n",
            "✓ Cleanup complete!\n",
            "\n",
            "================================================================================\n",
            "All Phase 3 optimizations successfully applied!\n",
            "================================================================================\n",
            "/content/drive/MyDrive/Tài liệu HCMUS/Năm 4/ltss/Doan/Autoencoder-based-unsupervised-feature-learning-system\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uEU1nsvTKjvX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}