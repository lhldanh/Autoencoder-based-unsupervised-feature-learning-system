{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecdbf77d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T14:01:32.238453Z",
     "iopub.status.busy": "2025-12-14T14:01:32.238159Z",
     "iopub.status.idle": "2025-12-14T14:01:32.242312Z",
     "shell.execute_reply": "2025-12-14T14:01:32.241630Z"
    },
    "id": "1fTKcxHDDoIe",
    "outputId": "b22bc054-bfc8-471e-8d18-3bc63985c456",
    "papermill": {
     "duration": 0.010716,
     "end_time": "2025-12-14T14:01:32.243451",
     "exception": false,
     "start_time": "2025-12-14T14:01:32.232735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "180b7bce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T14:01:32.251277Z",
     "iopub.status.busy": "2025-12-14T14:01:32.250841Z",
     "iopub.status.idle": "2025-12-14T14:01:32.253940Z",
     "shell.execute_reply": "2025-12-14T14:01:32.253384Z"
    },
    "id": "Stt58Qna-O09",
    "outputId": "cca2fdff-34af-4eef-fa98-947d21fadb5e",
    "papermill": {
     "duration": 0.008063,
     "end_time": "2025-12-14T14:01:32.254996",
     "exception": false,
     "start_time": "2025-12-14T14:01:32.246933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %cd drive/MyDrive/\"Tài liệu HCMUS\"/'Năm 4'/ltss/Doan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d2fe726",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T14:01:32.262485Z",
     "iopub.status.busy": "2025-12-14T14:01:32.262125Z",
     "iopub.status.idle": "2025-12-14T14:01:33.046314Z",
     "shell.execute_reply": "2025-12-14T14:01:33.045436Z"
    },
    "id": "ptsf8xVWF_JJ",
    "outputId": "616d87b8-a00f-400d-a5a8-bcec4f90b096",
    "papermill": {
     "duration": 0.789505,
     "end_time": "2025-12-14T14:01:33.047736",
     "exception": false,
     "start_time": "2025-12-14T14:01:32.258231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Autoencoder-based-unsupervised-feature-learning-system'...\r\n",
      "remote: Enumerating objects: 211, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (211/211), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (142/142), done.\u001b[K\r\n",
      "remote: Total 211 (delta 120), reused 145 (delta 64), pack-reused 0 (from 0)\u001b[K\r\n",
      "Receiving objects: 100% (211/211), 116.83 KiB | 2.66 MiB/s, done.\r\n",
      "Resolving deltas: 100% (120/120), done.\r\n",
      "/kaggle/working/Autoencoder-based-unsupervised-feature-learning-system\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/lhldanh/Autoencoder-based-unsupervised-feature-learning-system.git\n",
    "%cd Autoencoder-based-unsupervised-feature-learning-system/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13699e94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T14:01:33.056483Z",
     "iopub.status.busy": "2025-12-14T14:01:33.056225Z",
     "iopub.status.idle": "2025-12-14T14:01:38.568529Z",
     "shell.execute_reply": "2025-12-14T14:01:38.567741Z"
    },
    "id": "0a0ff136",
    "papermill": {
     "duration": 5.51846,
     "end_time": "2025-12-14T14:01:38.570126",
     "exception": false,
     "start_time": "2025-12-14T14:01:33.051666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-14 14:01:33--  https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\r\n",
      "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\r\n",
      "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 170052171 (162M) [application/x-gzip]\r\n",
      "Saving to: ‘data/cifar-10-binary.tar.gz’\r\n",
      "\r\n",
      "data/cifar-10-binar 100%[===================>] 162.17M  55.3MB/s    in 2.9s    \r\n",
      "\r\n",
      "2025-12-14 14:01:36 (55.3 MB/s) - ‘data/cifar-10-binary.tar.gz’ saved [170052171/170052171]\r\n",
      "\r\n",
      "cifar-10-batches-bin/\r\n",
      "cifar-10-batches-bin/data_batch_1.bin\r\n",
      "cifar-10-batches-bin/batches.meta.txt\r\n",
      "cifar-10-batches-bin/data_batch_3.bin\r\n",
      "cifar-10-batches-bin/data_batch_4.bin\r\n",
      "cifar-10-batches-bin/test_batch.bin\r\n",
      "cifar-10-batches-bin/readme.html\r\n",
      "cifar-10-batches-bin/data_batch_5.bin\r\n",
      "cifar-10-batches-bin/data_batch_2.bin\r\n"
     ]
    }
   ],
   "source": [
    "%mkdir -p build\n",
    "%mkdir -p weights\n",
    "%mkdir -p data\n",
    "!wget https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz -O data/cifar-10-binary.tar.gz\n",
    "!tar -xzvf data/cifar-10-binary.tar.gz -C data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f216ffca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T14:01:38.581287Z",
     "iopub.status.busy": "2025-12-14T14:01:38.580661Z",
     "iopub.status.idle": "2025-12-14T14:01:38.699826Z",
     "shell.execute_reply": "2025-12-14T14:01:38.698930Z"
    },
    "id": "tOEubgcjB64H",
    "outputId": "aa26478b-16e5-448a-f2cd-fa628574c199",
    "papermill": {
     "duration": 0.12622,
     "end_time": "2025-12-14T14:01:38.701166",
     "exception": false,
     "start_time": "2025-12-14T14:01:38.574946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build  data  include  README.md  src  weights\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faa1ff77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T14:01:38.722700Z",
     "iopub.status.busy": "2025-12-14T14:01:38.722418Z",
     "iopub.status.idle": "2025-12-14T14:01:40.971238Z",
     "shell.execute_reply": "2025-12-14T14:01:40.970256Z"
    },
    "id": "njgQkXrK_k_1",
    "outputId": "cced8b29-51bf-4662-a967-b7280a6b92b8",
    "papermill": {
     "duration": 2.266716,
     "end_time": "2025-12-14T14:01:40.972670",
     "exception": false,
     "start_time": "2025-12-14T14:01:38.705954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "major, minor = cuda.get_current_device().compute_capability\n",
    "print(f'GPU compute capability: {major}.{minor}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5098a85",
   "metadata": {
    "id": "ZtZItoPTFQEa",
    "papermill": {
     "duration": 0.005024,
     "end_time": "2025-12-14T14:01:40.982952",
     "exception": false,
     "start_time": "2025-12-14T14:01:40.977928",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## writefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3384a4f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T14:01:40.994685Z",
     "iopub.status.busy": "2025-12-14T14:01:40.994422Z",
     "iopub.status.idle": "2025-12-14T14:01:41.013444Z",
     "shell.execute_reply": "2025-12-14T14:01:41.012792Z"
    },
    "id": "_Jrx_dwSFRkh",
    "outputId": "a4738c2b-e9b3-4320-ca6f-a7a05b8049bb",
    "papermill": {
     "duration": 0.026777,
     "end_time": "2025-12-14T14:01:41.014551",
     "exception": false,
     "start_time": "2025-12-14T14:01:40.987774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_gpu_optimize.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/train_gpu_optimize.cu\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "#include <random>\n",
    "#include <algorithm>\n",
    "#include <fstream>\n",
    "#include <chrono>\n",
    "#include <cmath>\n",
    "#include <iomanip>\n",
    "#include \"cifar10_dataset.h\"\n",
    "#include <cuda_runtime.h>\n",
    "#include <device_launch_parameters.h>\n",
    "\n",
    "#define BLOCK_SIZE 256\n",
    "#define TILE_SIZE 32\n",
    "#define GRID(n) ((n + BLOCK_SIZE - 1) / BLOCK_SIZE)\n",
    "\n",
    "// Optimized GEMM tile sizes\n",
    "#define TILE_M 64\n",
    "#define TILE_N 64\n",
    "#define TILE_K 16\n",
    "#define THREAD_M 4\n",
    "#define THREAD_N 4\n",
    "\n",
    "#define CUDA_CHECK(call) do { \\\n",
    "    cudaError_t err = call; \\\n",
    "    if (err != cudaSuccess) { \\\n",
    "        std::cerr << \"CUDA Error: \" << cudaGetErrorString(err) \\\n",
    "                  << \" at \" << __FILE__ << \":\" << __LINE__ << std::endl; \\\n",
    "        exit(err); \\\n",
    "    } \\\n",
    "} while(0)\n",
    "\n",
    "// ============== MEMORY POOL ==============\n",
    "class MemoryPool {\n",
    "    std::vector<std::pair<float*, size_t>> buffers;\n",
    "    size_t total = 0;\n",
    "public:\n",
    "    float* alloc(size_t bytes) {\n",
    "        float* p; cudaMalloc(&p, bytes);\n",
    "        buffers.push_back({p, bytes});\n",
    "        total += bytes;\n",
    "        return p;\n",
    "    }\n",
    "    size_t get_total() const { return total; }\n",
    "    ~MemoryPool() { for (auto& b : buffers) cudaFree(b.first); }\n",
    "};\n",
    "\n",
    "// ============== FUSED GEMM + BIAS + RELU KERNELS (FORWARD) ==============\n",
    "\n",
    "// C[M,N] = ReLU(A[M,K] * B^T[N,K] + bias[N])  (B is transposed, fused bias+relu)\n",
    "__global__ void gemm_nt_bias_relu_kernel(\n",
    "    const float* __restrict__ A,\n",
    "    const float* __restrict__ B,\n",
    "    const float* __restrict__ bias,\n",
    "    float* __restrict__ C,\n",
    "    int M, int K, int N, bool relu)\n",
    "{\n",
    "    __shared__ float As[TILE_SIZE][TILE_SIZE];\n",
    "    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n",
    "    \n",
    "    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n",
    "    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n",
    "    \n",
    "    float sum = 0.0f;\n",
    "    \n",
    "    int numTiles = (K + TILE_SIZE - 1) / TILE_SIZE;\n",
    "    \n",
    "    for (int t = 0; t < numTiles; ++t) {\n",
    "        int a_col = t * TILE_SIZE + threadIdx.x;\n",
    "        int b_col = t * TILE_SIZE + threadIdx.y;\n",
    "        \n",
    "        As[threadIdx.y][threadIdx.x] = (row < M && a_col < K) ? A[row * K + a_col] : 0.0f;\n",
    "        Bs[threadIdx.y][threadIdx.x] = (col < N && b_col < K) ? B[col * K + b_col] : 0.0f;\n",
    "        \n",
    "        __syncthreads();\n",
    "        \n",
    "        #pragma unroll\n",
    "        for (int k = 0; k < TILE_SIZE; ++k) {\n",
    "            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n",
    "        }\n",
    "        \n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    if (row < M && col < N) {\n",
    "        float val = sum + bias[col];\n",
    "        C[row * N + col] = relu ? fmaxf(val, 0.0f) : val;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Optimized version with register blocking - fused bias + relu\n",
    "__global__ void gemm_nt_bias_relu_optimized_kernel(\n",
    "    const float* __restrict__ A,\n",
    "    const float* __restrict__ B,\n",
    "    const float* __restrict__ bias,\n",
    "    float* __restrict__ C,\n",
    "    int M, int K, int N, bool relu)\n",
    "{\n",
    "    __shared__ float As[TILE_K][TILE_M];\n",
    "    __shared__ float Bs[TILE_K][TILE_N];\n",
    "    \n",
    "    int bx = blockIdx.x, by = blockIdx.y;\n",
    "    int tx = threadIdx.x, ty = threadIdx.y;\n",
    "    int tid = ty * blockDim.x + tx;\n",
    "    \n",
    "    float acc[THREAD_M][THREAD_N] = {0.0f};\n",
    "    \n",
    "    int row_base = by * TILE_M;\n",
    "    int col_base = bx * TILE_N;\n",
    "    \n",
    "    int threads_per_block = blockDim.x * blockDim.y;\n",
    "    \n",
    "    for (int k = 0; k < K; k += TILE_K) {\n",
    "        for (int i = tid; i < TILE_K * TILE_M; i += threads_per_block) {\n",
    "            int ki = i / TILE_M;\n",
    "            int mi = i % TILE_M;\n",
    "            int global_row = row_base + mi;\n",
    "            int global_k = k + ki;\n",
    "            As[ki][mi] = (global_row < M && global_k < K) ? A[global_row * K + global_k] : 0.0f;\n",
    "        }\n",
    "        \n",
    "        for (int i = tid; i < TILE_K * TILE_N; i += threads_per_block) {\n",
    "            int ki = i / TILE_N;\n",
    "            int ni = i % TILE_N;\n",
    "            int global_col = col_base + ni;\n",
    "            int global_k = k + ki;\n",
    "            Bs[ki][ni] = (global_col < N && global_k < K) ? B[global_col * K + global_k] : 0.0f;\n",
    "        }\n",
    "        \n",
    "        __syncthreads();\n",
    "        \n",
    "        #pragma unroll\n",
    "        for (int ki = 0; ki < TILE_K; ++ki) {\n",
    "            float a_reg[THREAD_M], b_reg[THREAD_N];\n",
    "            \n",
    "            #pragma unroll\n",
    "            for (int m = 0; m < THREAD_M; ++m) {\n",
    "                a_reg[m] = As[ki][ty * THREAD_M + m];\n",
    "            }\n",
    "            #pragma unroll\n",
    "            for (int n = 0; n < THREAD_N; ++n) {\n",
    "                b_reg[n] = Bs[ki][tx * THREAD_N + n];\n",
    "            }\n",
    "            \n",
    "            #pragma unroll\n",
    "            for (int m = 0; m < THREAD_M; ++m) {\n",
    "                #pragma unroll\n",
    "                for (int n = 0; n < THREAD_N; ++n) {\n",
    "                    acc[m][n] += a_reg[m] * b_reg[n];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    #pragma unroll\n",
    "    for (int m = 0; m < THREAD_M; ++m) {\n",
    "        int global_row = row_base + ty * THREAD_M + m;\n",
    "        #pragma unroll\n",
    "        for (int n = 0; n < THREAD_N; ++n) {\n",
    "            int global_col = col_base + tx * THREAD_N + n;\n",
    "            if (global_row < M && global_col < N) {\n",
    "                float val = acc[m][n] + bias[global_col];\n",
    "                C[global_row * N + global_col] = relu ? fmaxf(val, 0.0f) : val;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// Wrapper with automatic kernel selection for fused GEMM+bias+relu\n",
    "void gemm_nt_bias_relu(const float* A, const float* B, const float* bias, \n",
    "                        float* C, int M, int K, int N, bool relu, cudaStream_t stream) {\n",
    "    if (M >= 64 && N >= 64 && K >= 16) {\n",
    "        dim3 block(TILE_N / THREAD_N, TILE_M / THREAD_M);\n",
    "        dim3 grid((N + TILE_N - 1) / TILE_N, (M + TILE_M - 1) / TILE_M);\n",
    "        gemm_nt_bias_relu_optimized_kernel<<<grid, block, 0, stream>>>(A, B, bias, C, M, K, N, relu);\n",
    "    } else {\n",
    "        dim3 block(TILE_SIZE, TILE_SIZE);\n",
    "        dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n",
    "        gemm_nt_bias_relu_kernel<<<grid, block, 0, stream>>>(A, B, bias, C, M, K, N, relu);\n",
    "    }\n",
    "}\n",
    "\n",
    "// ============== STANDARD GEMM KERNELS ==============\n",
    "\n",
    "// C[M,N] = A[M,K] * B[K,N]\n",
    "__global__ void gemm_nn_kernel(\n",
    "    const float* __restrict__ A,\n",
    "    const float* __restrict__ B,\n",
    "    float* __restrict__ C,\n",
    "    int M, int K, int N)\n",
    "{\n",
    "    __shared__ float As[TILE_SIZE][TILE_SIZE];\n",
    "    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n",
    "    \n",
    "    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n",
    "    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n",
    "    \n",
    "    float sum = 0.0f;\n",
    "    \n",
    "    int numTiles = (K + TILE_SIZE - 1) / TILE_SIZE;\n",
    "    \n",
    "    for (int t = 0; t < numTiles; ++t) {\n",
    "        int a_col = t * TILE_SIZE + threadIdx.x;\n",
    "        int b_row = t * TILE_SIZE + threadIdx.y;\n",
    "        \n",
    "        As[threadIdx.y][threadIdx.x] = (row < M && a_col < K) ? A[row * K + a_col] : 0.0f;\n",
    "        Bs[threadIdx.y][threadIdx.x] = (b_row < K && col < N) ? B[b_row * N + col] : 0.0f;\n",
    "        \n",
    "        __syncthreads();\n",
    "        \n",
    "        #pragma unroll\n",
    "        for (int k = 0; k < TILE_SIZE; ++k) {\n",
    "            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n",
    "        }\n",
    "        \n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    if (row < M && col < N) {\n",
    "        C[row * N + col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "// C[M,N] = A^T[K,M] * B[K,N]  (A is transposed)\n",
    "__global__ void gemm_tn_kernel(\n",
    "    const float* __restrict__ A,\n",
    "    const float* __restrict__ B,\n",
    "    float* __restrict__ C,\n",
    "    int M, int K, int N)\n",
    "{\n",
    "    __shared__ float As[TILE_SIZE][TILE_SIZE];\n",
    "    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n",
    "    \n",
    "    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n",
    "    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n",
    "    \n",
    "    float sum = 0.0f;\n",
    "    \n",
    "    int numTiles = (K + TILE_SIZE - 1) / TILE_SIZE;\n",
    "    \n",
    "    for (int t = 0; t < numTiles; ++t) {\n",
    "        int a_row = t * TILE_SIZE + threadIdx.x;\n",
    "        int b_row = t * TILE_SIZE + threadIdx.y;\n",
    "        \n",
    "        As[threadIdx.y][threadIdx.x] = (a_row < K && row < M) ? A[a_row * M + row] : 0.0f;\n",
    "        Bs[threadIdx.y][threadIdx.x] = (b_row < K && col < N) ? B[b_row * N + col] : 0.0f;\n",
    "        \n",
    "        __syncthreads();\n",
    "        \n",
    "        #pragma unroll\n",
    "        for (int k = 0; k < TILE_SIZE; ++k) {\n",
    "            sum += As[k][threadIdx.y] * Bs[k][threadIdx.x];\n",
    "        }\n",
    "        \n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    if (row < M && col < N) {\n",
    "        C[row * N + col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "void gemm_nn(const float* A, const float* B, float* C, int M, int K, int N, cudaStream_t stream) {\n",
    "    dim3 block(TILE_SIZE, TILE_SIZE);\n",
    "    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n",
    "    gemm_nn_kernel<<<grid, block, 0, stream>>>(A, B, C, M, K, N);\n",
    "}\n",
    "\n",
    "void gemm_tn(const float* A, const float* B, float* C, int M, int K, int N, cudaStream_t stream) {\n",
    "    dim3 block(TILE_SIZE, TILE_SIZE);\n",
    "    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n",
    "    gemm_tn_kernel<<<grid, block, 0, stream>>>(A, B, C, M, K, N);\n",
    "}\n",
    "\n",
    "// ============== IM2COL KERNEL ==============\n",
    "__global__ void im2col_kernel(\n",
    "    const float* __restrict__ input,\n",
    "    float* __restrict__ col,\n",
    "    int B, int H, int W, int C,\n",
    "    int K, int P, int H_out, int W_out)\n",
    "{\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int total = B * H_out * W_out * C * K * K;\n",
    "    if (idx >= total) return;\n",
    "    \n",
    "    int kk = idx % (K * K);\n",
    "    int tmp = idx / (K * K);\n",
    "    int c = tmp % C;\n",
    "    tmp /= C;\n",
    "    int ow = tmp % W_out;\n",
    "    tmp /= W_out;\n",
    "    int oh = tmp % H_out;\n",
    "    int b = tmp / H_out;\n",
    "    \n",
    "    int kh = kk / K;\n",
    "    int kw = kk % K;\n",
    "    int ih = oh - P + kh;\n",
    "    int iw = ow - P + kw;\n",
    "    \n",
    "    int col_row = b * (H_out * W_out) + oh * W_out + ow;\n",
    "    int col_col = c * K * K + kh * K + kw;\n",
    "    int col_width = C * K * K;\n",
    "    \n",
    "    float val = 0.0f;\n",
    "    if (ih >= 0 && ih < H && iw >= 0 && iw < W) {\n",
    "        val = input[b * (H * W * C) + ih * (W * C) + iw * C + c];\n",
    "    }\n",
    "    col[col_row * col_width + col_col] = val;\n",
    "}\n",
    "\n",
    "// ============== COL2IM KERNEL ==============\n",
    "__global__ void col2im_kernel(\n",
    "    const float* __restrict__ col,\n",
    "    float* __restrict__ input_grad,\n",
    "    int B, int H, int W, int C,\n",
    "    int K, int P, int H_out, int W_out)\n",
    "{\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int total = B * H * W * C;\n",
    "    if (idx >= total) return;\n",
    "    \n",
    "    int ic = idx % C;\n",
    "    int tmp = idx / C;\n",
    "    int iw = tmp % W;\n",
    "    tmp /= W;\n",
    "    int ih = tmp % H;\n",
    "    int b = tmp / H;\n",
    "    \n",
    "    float sum = 0.0f;\n",
    "    int col_width = C * K * K;\n",
    "    \n",
    "    #pragma unroll\n",
    "    for (int kh = 0; kh < K; ++kh) {\n",
    "        #pragma unroll\n",
    "        for (int kw = 0; kw < K; ++kw) {\n",
    "            int oh = ih + P - kh;\n",
    "            int ow = iw + P - kw;\n",
    "            \n",
    "            if (oh >= 0 && oh < H_out && ow >= 0 && ow < W_out) {\n",
    "                int col_row = b * (H_out * W_out) + oh * W_out + ow;\n",
    "                int col_col = ic * K * K + kh * K + kw;\n",
    "                sum += col[col_row * col_width + col_col];\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    input_grad[idx] = sum;\n",
    "}\n",
    "\n",
    "// ============== FUSED MAXPOOL ==============\n",
    "__global__ void maxpool_kernel(\n",
    "    const float* __restrict__ input,\n",
    "    float* __restrict__ output,\n",
    "    int* __restrict__ indices,\n",
    "    int B, int H_in, int W_in, int C)\n",
    "{\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int H_out = H_in / 2, W_out = W_in / 2;\n",
    "    int total = B * H_out * W_out * C;\n",
    "    if (idx >= total) return;\n",
    "    \n",
    "    int c = idx % C;\n",
    "    int tmp = idx / C;\n",
    "    int wo = tmp % W_out;\n",
    "    tmp /= W_out;\n",
    "    int ho = tmp % H_out;\n",
    "    int b = tmp / H_out;\n",
    "    \n",
    "    int hi = ho * 2, wi = wo * 2;\n",
    "    float max_val = -1e10f;\n",
    "    int max_idx = 0;\n",
    "    \n",
    "    #pragma unroll\n",
    "    for (int dh = 0; dh < 2; ++dh) {\n",
    "        #pragma unroll\n",
    "        for (int dw = 0; dw < 2; ++dw) {\n",
    "            int in_idx = b * (H_in * W_in * C) + (hi + dh) * (W_in * C) + (wi + dw) * C + c;\n",
    "            float v = input[in_idx];\n",
    "            if (v > max_val) { max_val = v; max_idx = in_idx; }\n",
    "        }\n",
    "    }\n",
    "    output[idx] = max_val;\n",
    "    indices[idx] = max_idx;\n",
    "}\n",
    "\n",
    "// ============== UPSAMPLE ==============\n",
    "__global__ void upsample_kernel(\n",
    "    const float* __restrict__ input,\n",
    "    float* __restrict__ output,\n",
    "    int B, int H_in, int W_in, int C)\n",
    "{\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int H_out = H_in * 2, W_out = W_in * 2;\n",
    "    int total = B * H_out * W_out * C;\n",
    "    if (idx >= total) return;\n",
    "    \n",
    "    int c = idx % C;\n",
    "    int tmp = idx / C;\n",
    "    int wo = tmp % W_out;\n",
    "    tmp /= W_out;\n",
    "    int ho = tmp % H_out;\n",
    "    int b = tmp / H_out;\n",
    "    \n",
    "    output[idx] = input[b * (H_in * W_in * C) + (ho / 2) * (W_in * C) + (wo / 2) * C + c];\n",
    "}\n",
    "\n",
    "// ============== FUSED BACKWARD KERNELS ==============\n",
    "\n",
    "// Fused MSE loss + backward in single kernel\n",
    "__global__ void mse_loss_backward_fused_kernel(\n",
    "    const float* __restrict__ pred,\n",
    "    const float* __restrict__ target,\n",
    "    float* __restrict__ grad,\n",
    "    float* __restrict__ partial_loss,\n",
    "    int size)\n",
    "{\n",
    "    __shared__ float s[256];\n",
    "    int tid = threadIdx.x;\n",
    "    int idx = blockIdx.x * blockDim.x + tid;\n",
    "    float local_sum = 0.0f;\n",
    "    float inv_size = 2.0f / size;\n",
    "    \n",
    "    for (int i = idx; i < size; i += blockDim.x * gridDim.x) {\n",
    "        float d = pred[i] - target[i];\n",
    "        grad[i] = d * inv_size;\n",
    "        local_sum += d * d;\n",
    "    }\n",
    "    \n",
    "    s[tid] = local_sum;\n",
    "    __syncthreads();\n",
    "    \n",
    "    for (int i = 128; i > 0; i >>= 1) {\n",
    "        if (tid < i) s[tid] += s[tid + i];\n",
    "        __syncthreads();\n",
    "    }\n",
    "    if (tid == 0) atomicAdd(partial_loss, s[0]);\n",
    "}\n",
    "\n",
    "// ============== FUSED UPSAMPLE + RELU BACKWARD ==============\n",
    "__global__ void fused_upsample_relu_backward_kernel(\n",
    "    const float* __restrict__ d_out,\n",
    "    const float* __restrict__ fwd,\n",
    "    float* __restrict__ d_in,\n",
    "    int B, int H_in, int W_in, int C)\n",
    "{\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int total = B * H_in * W_in * C;\n",
    "    if (idx >= total) return;\n",
    "    \n",
    "    int c = idx % C;\n",
    "    int tmp = idx / C;\n",
    "    int wi = tmp % W_in;\n",
    "    tmp /= W_in;\n",
    "    int hi = tmp % H_in;\n",
    "    int b = tmp / H_in;\n",
    "    \n",
    "    int H_out = H_in * 2, W_out = W_in * 2;\n",
    "    int ho = hi * 2, wo = wi * 2;\n",
    "    \n",
    "    // Upsample backward: sum 2x2 region\n",
    "    float sum = d_out[b * (H_out * W_out * C) + ho * (W_out * C) + wo * C + c]\n",
    "              + d_out[b * (H_out * W_out * C) + ho * (W_out * C) + (wo + 1) * C + c]\n",
    "              + d_out[b * (H_out * W_out * C) + (ho + 1) * (W_out * C) + wo * C + c]\n",
    "              + d_out[b * (H_out * W_out * C) + (ho + 1) * (W_out * C) + (wo + 1) * C + c];\n",
    "    \n",
    "    // Fused ReLU backward\n",
    "    d_in[idx] = (fwd[idx] > 0.0f) ? sum : 0.0f;\n",
    "}\n",
    "\n",
    "void fused_upsample_relu_backward(const float* d_out, const float* fwd, float* d_in,\n",
    "                                   int B, int H_in, int W_in, int C, cudaStream_t stream) {\n",
    "    int total = B * H_in * W_in * C;\n",
    "    fused_upsample_relu_backward_kernel<<<GRID(total), BLOCK_SIZE, 0, stream>>>(\n",
    "        d_out, fwd, d_in, B, H_in, W_in, C);\n",
    "}\n",
    "\n",
    "\n",
    "// ============== VECTORIZED UTILITY KERNELS ==============\n",
    "\n",
    "__global__ void fill_zeros_vectorized_kernel(float* data, int size) {\n",
    "    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * 4;\n",
    "    if (idx + 3 < size) {\n",
    "        *reinterpret_cast<float4*>(&data[idx]) = make_float4(0.0f, 0.0f, 0.0f, 0.0f);\n",
    "    } else if (idx < size) {\n",
    "        for (int i = idx; i < size; ++i) data[i] = 0.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void sgd_vectorized_kernel(float* w, const float* g, int size, float lr) {\n",
    "    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * 4;\n",
    "    if (idx + 3 < size) {\n",
    "        float4 w4 = *reinterpret_cast<float4*>(&w[idx]);\n",
    "        float4 g4 = *reinterpret_cast<const float4*>(&g[idx]);\n",
    "        w4.x -= lr * g4.x;\n",
    "        w4.y -= lr * g4.y;\n",
    "        w4.z -= lr * g4.z;\n",
    "        w4.w -= lr * g4.w;\n",
    "        *reinterpret_cast<float4*>(&w[idx]) = w4;\n",
    "    } else if (idx < size) {\n",
    "        for (int i = idx; i < size; ++i) {\n",
    "            w[i] -= lr * g[i];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void sgd_kernel(float* w, const float* g, int size, float lr) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < size) w[i] -= lr * g[i];\n",
    "}\n",
    "\n",
    "// ============== FUSED MAXPOOL + RELU BACKWARD ==============\n",
    "__global__ void fused_maxpool_relu_backward_kernel(\n",
    "    const float* __restrict__ d_out,\n",
    "    const int* __restrict__ indices,\n",
    "    const float* __restrict__ fwd,\n",
    "    float* __restrict__ d_in,\n",
    "    int pool_size)\n",
    "{\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i >= pool_size) return;\n",
    "    \n",
    "    int target_idx = indices[i];\n",
    "    float grad = d_out[i];\n",
    "    \n",
    "    // Fused ReLU backward: only propagate if forward was > 0\n",
    "    if (fwd[target_idx] > 0.0f) {\n",
    "        atomicAdd(&d_in[target_idx], grad);\n",
    "    }\n",
    "}\n",
    "\n",
    "void fused_maxpool_relu_backward(const float* d_out, const int* indices, const float* fwd,\n",
    "                                  float* d_in, int pool_size, int input_size, cudaStream_t stream) {\n",
    "    // First zero d_in\n",
    "    fill_zeros_vectorized_kernel<<<GRID(input_size / 4), BLOCK_SIZE, 0, stream>>>(d_in, input_size);\n",
    "    // Then scatter with fused relu\n",
    "    fused_maxpool_relu_backward_kernel<<<GRID(pool_size), BLOCK_SIZE, 0, stream>>>(\n",
    "        d_out, indices, fwd, d_in, pool_size);\n",
    "}\n",
    "\n",
    "// ============== FUSED GEMM_NN + RELU BACKWARD (for input gradient) ==============\n",
    "// Computes: d_col = ReLU_backward(d_out, fwd) * W\n",
    "__global__ void gemm_nn_relu_backward_kernel(\n",
    "    const float* __restrict__ d_out,\n",
    "    const float* __restrict__ fwd,\n",
    "    const float* __restrict__ W,\n",
    "    float* __restrict__ d_col,\n",
    "    int M, int K, int N)\n",
    "{\n",
    "    __shared__ float As[TILE_SIZE][TILE_SIZE];\n",
    "    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n",
    "    \n",
    "    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n",
    "    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n",
    "    \n",
    "    float sum = 0.0f;\n",
    "    int numTiles = (K + TILE_SIZE - 1) / TILE_SIZE;\n",
    "    \n",
    "    for (int t = 0; t < numTiles; ++t) {\n",
    "        int a_col = t * TILE_SIZE + threadIdx.x;\n",
    "        int b_row = t * TILE_SIZE + threadIdx.y;\n",
    "        \n",
    "        // Load A with ReLU backward mask applied\n",
    "        float a_val = 0.0f;\n",
    "        if (row < M && a_col < K) {\n",
    "            float fwd_val = fwd[row * K + a_col];\n",
    "            float grad_val = d_out[row * K + a_col];\n",
    "            a_val = (fwd_val > 0.0f) ? grad_val : 0.0f;\n",
    "        }\n",
    "        As[threadIdx.y][threadIdx.x] = a_val;\n",
    "        Bs[threadIdx.y][threadIdx.x] = (b_row < K && col < N) ? W[b_row * N + col] : 0.0f;\n",
    "        \n",
    "        __syncthreads();\n",
    "        \n",
    "        #pragma unroll\n",
    "        for (int k = 0; k < TILE_SIZE; ++k) {\n",
    "            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n",
    "        }\n",
    "        \n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    if (row < M && col < N) {\n",
    "        d_col[row * N + col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "void gemm_nn_relu_backward(const float* d_out, const float* fwd, const float* W,\n",
    "                            float* d_col, int M, int K, int N, cudaStream_t stream) {\n",
    "    dim3 block(TILE_SIZE, TILE_SIZE);\n",
    "    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n",
    "    gemm_nn_relu_backward_kernel<<<grid, block, 0, stream>>>(d_out, fwd, W, d_col, M, K, N);\n",
    "}\n",
    "\n",
    "// ============== FUSED GEMM_TN + RELU BACKWARD (for weight gradient) ==============\n",
    "// Computes: dW = (ReLU_backward(d_out, fwd))^T * col\n",
    "__global__ void gemm_tn_relu_backward_kernel(\n",
    "    const float* __restrict__ d_out,\n",
    "    const float* __restrict__ fwd,\n",
    "    const float* __restrict__ col,\n",
    "    float* __restrict__ dW,\n",
    "    int M, int K, int N)\n",
    "{\n",
    "    __shared__ float As[TILE_SIZE][TILE_SIZE];\n",
    "    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n",
    "    \n",
    "    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n",
    "    int col_idx = blockIdx.x * TILE_SIZE + threadIdx.x;\n",
    "    \n",
    "    float sum = 0.0f;\n",
    "    int numTiles = (K + TILE_SIZE - 1) / TILE_SIZE;\n",
    "    \n",
    "    for (int t = 0; t < numTiles; ++t) {\n",
    "        int a_row = t * TILE_SIZE + threadIdx.x;\n",
    "        int b_row = t * TILE_SIZE + threadIdx.y;\n",
    "        \n",
    "        // Load A^T with ReLU backward mask\n",
    "        float a_val = 0.0f;\n",
    "        if (a_row < K && row < M) {\n",
    "            float fwd_val = fwd[a_row * M + row];\n",
    "            float grad_val = d_out[a_row * M + row];\n",
    "            a_val = (fwd_val > 0.0f) ? grad_val : 0.0f;\n",
    "        }\n",
    "        As[threadIdx.y][threadIdx.x] = a_val;\n",
    "        Bs[threadIdx.y][threadIdx.x] = (b_row < K && col_idx < N) ? col[b_row * N + col_idx] : 0.0f;\n",
    "        \n",
    "        __syncthreads();\n",
    "        \n",
    "        #pragma unroll\n",
    "        for (int k = 0; k < TILE_SIZE; ++k) {\n",
    "            sum += As[k][threadIdx.y] * Bs[k][threadIdx.x];\n",
    "        }\n",
    "        \n",
    "        __syncthreads();\n",
    "    }\n",
    "    \n",
    "    if (row < M && col_idx < N) {\n",
    "        dW[row * N + col_idx] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "void gemm_tn_relu_backward(const float* d_out, const float* fwd, const float* col,\n",
    "                            float* dW, int M, int K, int N, cudaStream_t stream) {\n",
    "    dim3 block(TILE_SIZE, TILE_SIZE);\n",
    "    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n",
    "    gemm_tn_relu_backward_kernel<<<grid, block, 0, stream>>>(d_out, fwd, col, dW, M, K, N);\n",
    "}\n",
    "\n",
    "// ============== FUSED BIAS BACKWARD + RELU ==============\n",
    "__global__ void bias_backward_relu_kernel(\n",
    "    const float* __restrict__ d_out,\n",
    "    const float* __restrict__ fwd,\n",
    "    float* __restrict__ d_bias,\n",
    "    int B_HW, int C)\n",
    "{\n",
    "    __shared__ float shared_sum[BLOCK_SIZE];\n",
    "    \n",
    "    int oc = blockIdx.x;\n",
    "    if (oc >= C) return;\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    float local_sum = 0.0f;\n",
    "    \n",
    "    for (int i = tid; i < B_HW; i += BLOCK_SIZE) {\n",
    "        int idx = i * C + oc;\n",
    "        float fwd_val = fwd[idx];\n",
    "        float grad_val = d_out[idx];\n",
    "        local_sum += (fwd_val > 0.0f) ? grad_val : 0.0f;\n",
    "    }\n",
    "    \n",
    "    shared_sum[tid] = local_sum;\n",
    "    __syncthreads();\n",
    "    \n",
    "    if (tid < 128) shared_sum[tid] += shared_sum[tid + 128]; __syncthreads();\n",
    "    if (tid < 64) shared_sum[tid] += shared_sum[tid + 64]; __syncthreads();\n",
    "    \n",
    "    if (tid < 32) {\n",
    "        volatile float* vs = shared_sum;\n",
    "        vs[tid] += vs[tid + 32];\n",
    "        vs[tid] += vs[tid + 16];\n",
    "        vs[tid] += vs[tid + 8];\n",
    "        vs[tid] += vs[tid + 4];\n",
    "        vs[tid] += vs[tid + 2];\n",
    "        vs[tid] += vs[tid + 1];\n",
    "    }\n",
    "    \n",
    "    if (tid == 0) d_bias[oc] = shared_sum[0];\n",
    "}\n",
    "\n",
    "// ============== NON-FUSED BACKWARD KERNELS (for layers without ReLU) ==============\n",
    "\n",
    "__global__ void relu_backward_kernel(const float* d_out, const float* fwd, float* d_in, int size) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < size) d_in[i] = (fwd[i] > 0.0f) ? d_out[i] : 0.0f;\n",
    "}\n",
    "\n",
    "__global__ void maxpool_backward_kernel(const float* d_out, const int* idx, float* d_in, int size) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < size) atomicAdd(&d_in[idx[i]], d_out[i]);\n",
    "}\n",
    "\n",
    "__global__ void upsample_backward_kernel(const float* d_out, float* d_in, int B, int H_in, int W_in, int C) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int total = B * H_in * W_in * C;\n",
    "    if (idx >= total) return;\n",
    "    \n",
    "    int c = idx % C;\n",
    "    int tmp = idx / C;\n",
    "    int wi = tmp % W_in;\n",
    "    tmp /= W_in;\n",
    "    int hi = tmp % H_in;\n",
    "    int b = tmp / H_in;\n",
    "    \n",
    "    int H_out = H_in * 2, W_out = W_in * 2;\n",
    "    int ho = hi * 2, wo = wi * 2;\n",
    "    \n",
    "    float sum = d_out[b * (H_out * W_out * C) + ho * (W_out * C) + wo * C + c]\n",
    "              + d_out[b * (H_out * W_out * C) + ho * (W_out * C) + (wo + 1) * C + c]\n",
    "              + d_out[b * (H_out * W_out * C) + (ho + 1) * (W_out * C) + wo * C + c]\n",
    "              + d_out[b * (H_out * W_out * C) + (ho + 1) * (W_out * C) + (wo + 1) * C + c];\n",
    "    d_in[idx] = sum;\n",
    "}\n",
    "\n",
    "__global__ void bias_backward_kernel(const float* d_out, float* d_bias, int B_HW, int C) {\n",
    "    __shared__ float shared_sum[BLOCK_SIZE];\n",
    "    \n",
    "    int oc = blockIdx.x;\n",
    "    if (oc >= C) return;\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    float local_sum = 0.0f;\n",
    "    \n",
    "    for (int i = tid; i < B_HW; i += BLOCK_SIZE) {\n",
    "        local_sum += d_out[i * C + oc];\n",
    "    }\n",
    "    \n",
    "    shared_sum[tid] = local_sum;\n",
    "    __syncthreads();\n",
    "    \n",
    "    if (tid < 128) shared_sum[tid] += shared_sum[tid + 128]; __syncthreads();\n",
    "    if (tid < 64) shared_sum[tid] += shared_sum[tid + 64]; __syncthreads();\n",
    "    \n",
    "    if (tid < 32) {\n",
    "        volatile float* vs = shared_sum;\n",
    "        vs[tid] += vs[tid + 32];\n",
    "        vs[tid] += vs[tid + 16];\n",
    "        vs[tid] += vs[tid + 8];\n",
    "        vs[tid] += vs[tid + 4];\n",
    "        vs[tid] += vs[tid + 2];\n",
    "        vs[tid] += vs[tid + 1];\n",
    "    }\n",
    "    \n",
    "    if (tid == 0) d_bias[oc] = shared_sum[0];\n",
    "}\n",
    "\n",
    "void init_random(std::vector<float>& v, int fan_in, int fan_out) {\n",
    "    std::random_device rd;\n",
    "    std::mt19937 gen(rd());\n",
    "    float std_dev = std::sqrt(2.0f / (fan_in + fan_out));\n",
    "    std::normal_distribution<float> dist(0.0f, std_dev);\n",
    "    for (auto& x : v) x = dist(gen);\n",
    "}\n",
    "\n",
    "\n",
    "void save_weights(const std::string& f, const std::vector<float>& d) {\n",
    "    std::ofstream file(f, std::ios::binary);\n",
    "    uint32_t sz = d.size();\n",
    "    file.write((char*)&sz, 4);\n",
    "    file.write((char*)d.data(), d.size() * 4);\n",
    "}\n",
    "\n",
    "// ...existing code (all kernels and helper functions)...\n",
    "\n",
    "int main() {\n",
    "    const int B = 64, EPOCHS = 40;\n",
    "    const float LR = 0.01f;\n",
    "    \n",
    "    std::cout << \"=== CUDA Autoencoder (Fused Backward Kernels) ===\\n\\n\";\n",
    "    \n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, 0);\n",
    "    std::cout << \"GPU: \" << prop.name << \"\\n\";\n",
    "    std::cout << \"SMs: \" << prop.multiProcessorCount << \"\\n\\n\";\n",
    "    \n",
    "    CIFAR10Dataset dataset(\"../data/cifar-10-batches-bin\");\n",
    "    dataset.load_data();\n",
    "    if (dataset.get_num_train() == 0) { std::cerr << \"No data!\\n\"; return 1; }\n",
    "    std::cout << \"Images: \" << dataset.get_num_train() << \"\\n\\n\";\n",
    "    \n",
    "    MemoryPool pool;\n",
    "    \n",
    "    // Layer dimensions\n",
    "    int s_in = B * 32 * 32 * 3;\n",
    "    int s_l1 = B * 32 * 32 * 256, s_p1 = B * 16 * 16 * 256;\n",
    "    int s_l2 = B * 16 * 16 * 128, s_p2 = B * 8 * 8 * 128;\n",
    "    int s_l3 = B * 8 * 8 * 128,   s_u3 = B * 16 * 16 * 128;\n",
    "    int s_l4 = B * 16 * 16 * 256, s_u4 = B * 32 * 32 * 256;\n",
    "    \n",
    "    // Col buffer sizes\n",
    "    int col1_size = B * 32 * 32 * (3 * 9);\n",
    "    int col2_size = B * 16 * 16 * (256 * 9);\n",
    "    int col3_size = B * 8 * 8 * (128 * 9);\n",
    "    int col4_size = B * 16 * 16 * (128 * 9);\n",
    "    int col5_size = B * 32 * 32 * (256 * 9);\n",
    "    \n",
    "    // Weights\n",
    "    std::vector<float> h_w1(256 * 3 * 9), h_b1(256, 0);\n",
    "    std::vector<float> h_w2(128 * 256 * 9), h_b2(128, 0);\n",
    "    std::vector<float> h_w3(128 * 128 * 9), h_b3(128, 0);\n",
    "    std::vector<float> h_w4(256 * 128 * 9), h_b4(256, 0);\n",
    "    std::vector<float> h_w5(3 * 256 * 9), h_b5(3, 0);\n",
    "    \n",
    "    init_random(h_w1, 27, 256); init_random(h_w2, 2304, 128);\n",
    "    init_random(h_w3, 1152, 128); init_random(h_w4, 1152, 256);\n",
    "    init_random(h_w5, 2304, 3);\n",
    "    \n",
    "    // Device memory - weights\n",
    "    float *d_w1, *d_b1, *d_dw1, *d_db1;\n",
    "    float *d_w2, *d_b2, *d_dw2, *d_db2;\n",
    "    float *d_w3, *d_b3, *d_dw3, *d_db3;\n",
    "    float *d_w4, *d_b4, *d_dw4, *d_db4;\n",
    "    float *d_w5, *d_b5, *d_dw5, *d_db5;\n",
    "    \n",
    "    d_w1 = pool.alloc(h_w1.size() * 4); d_b1 = pool.alloc(256 * 4);\n",
    "    d_dw1 = pool.alloc(h_w1.size() * 4); d_db1 = pool.alloc(256 * 4);\n",
    "    d_w2 = pool.alloc(h_w2.size() * 4); d_b2 = pool.alloc(128 * 4);\n",
    "    d_dw2 = pool.alloc(h_w2.size() * 4); d_db2 = pool.alloc(128 * 4);\n",
    "    d_w3 = pool.alloc(h_w3.size() * 4); d_b3 = pool.alloc(128 * 4);\n",
    "    d_dw3 = pool.alloc(h_w3.size() * 4); d_db3 = pool.alloc(128 * 4);\n",
    "    d_w4 = pool.alloc(h_w4.size() * 4); d_b4 = pool.alloc(256 * 4);\n",
    "    d_dw4 = pool.alloc(h_w4.size() * 4); d_db4 = pool.alloc(256 * 4);\n",
    "    d_w5 = pool.alloc(h_w5.size() * 4); d_b5 = pool.alloc(3 * 4);\n",
    "    d_dw5 = pool.alloc(h_w5.size() * 4); d_db5 = pool.alloc(3 * 4);\n",
    "    \n",
    "    // Double buffering for input\n",
    "    float *d_input[2];\n",
    "    d_input[0] = pool.alloc(s_in * 4);\n",
    "    d_input[1] = pool.alloc(s_in * 4);\n",
    "    \n",
    "    // Forward buffers\n",
    "    float *d_l1, *d_p1, *d_l2, *d_p2, *d_l3, *d_u3, *d_l4, *d_u4, *d_out;\n",
    "    d_l1 = pool.alloc(s_l1 * 4); d_p1 = pool.alloc(s_p1 * 4);\n",
    "    d_l2 = pool.alloc(s_l2 * 4); d_p2 = pool.alloc(s_p2 * 4);\n",
    "    d_l3 = pool.alloc(s_l3 * 4); d_u3 = pool.alloc(s_u3 * 4);\n",
    "    d_l4 = pool.alloc(s_l4 * 4); d_u4 = pool.alloc(s_u4 * 4);\n",
    "    d_out = pool.alloc(s_in * 4);\n",
    "    \n",
    "    // Im2col buffers\n",
    "    float *d_col1, *d_col2, *d_col3, *d_col4, *d_col5;\n",
    "    d_col1 = pool.alloc(col1_size * 4);\n",
    "    d_col2 = pool.alloc(col2_size * 4);\n",
    "    d_col3 = pool.alloc(col3_size * 4);\n",
    "    d_col4 = pool.alloc(col4_size * 4);\n",
    "    d_col5 = pool.alloc(col5_size * 4);\n",
    "    \n",
    "    // Backward buffers\n",
    "    float *d_dl1, *d_dp1, *d_dl2, *d_dp2, *d_dl3, *d_du3, *d_dl4, *d_du4, *d_dout;\n",
    "    float *d_dcol;\n",
    "    d_dl1 = pool.alloc(s_l1 * 4); d_dp1 = pool.alloc(s_p1 * 4);\n",
    "    d_dl2 = pool.alloc(s_l2 * 4); d_dp2 = pool.alloc(s_p2 * 4);\n",
    "    d_dl3 = pool.alloc(s_l3 * 4); d_du3 = pool.alloc(s_u3 * 4);\n",
    "    d_dl4 = pool.alloc(s_l4 * 4); d_du4 = pool.alloc(s_u4 * 4);\n",
    "    d_dout = pool.alloc(s_in * 4);\n",
    "    d_dcol = pool.alloc(col5_size * 4);\n",
    "    \n",
    "    int *d_idx1 = (int*)pool.alloc(s_p1 * 4);\n",
    "    int *d_idx2 = (int*)pool.alloc(s_p2 * 4);\n",
    "    \n",
    "    float* d_loss = pool.alloc(4);\n",
    "    \n",
    "    std::cout << \"Memory: \" << pool.get_total() / (1024 * 1024) << \" MB\\n\\n\";\n",
    "    \n",
    "    // Pinned host memory\n",
    "    float* h_pinned_input;\n",
    "    cudaMallocHost(&h_pinned_input, s_in * 4);\n",
    "    \n",
    "    // Copy weights\n",
    "    cudaMemcpy(d_w1, h_w1.data(), h_w1.size() * 4, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_b1, h_b1.data(), 256 * 4, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_w2, h_w2.data(), h_w2.size() * 4, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_b2, h_b2.data(), 128 * 4, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_w3, h_w3.data(), h_w3.size() * 4, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_b3, h_b3.data(), 128 * 4, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_w4, h_w4.data(), h_w4.size() * 4, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_b4, h_b4.data(), 256 * 4, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_w5, h_w5.data(), h_w5.size() * 4, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_b5, h_b5.data(), 3 * 4, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    int num_batches = dataset.get_num_train() / B;\n",
    "    std::cout << \"Training: \" << EPOCHS << \" epochs, \" << num_batches << \" batches\\n\\n\";\n",
    "    \n",
    "    // Create streams\n",
    "    cudaStream_t stream_compute, stream_transfer;\n",
    "    cudaStreamCreate(&stream_compute);\n",
    "    cudaStreamCreate(&stream_transfer);\n",
    "    \n",
    "    auto t_start = std::chrono::high_resolution_clock::now();\n",
    "    \n",
    "    for (int epoch = 0; epoch < EPOCHS; ++epoch) {\n",
    "        auto ep_start = std::chrono::high_resolution_clock::now();\n",
    "        \n",
    "        cudaMemsetAsync(d_loss, 0, 4, stream_compute);\n",
    "        \n",
    "        // Pre-load first batch\n",
    "        memcpy(h_pinned_input, dataset.get_train_images_ptr(), s_in * 4);\n",
    "        cudaMemcpyAsync(d_input[0], h_pinned_input, s_in * 4, cudaMemcpyHostToDevice, stream_transfer);\n",
    "        \n",
    "        for (int batch = 0; batch < num_batches; ++batch) {\n",
    "            int curr_buf = batch % 2;\n",
    "            int next_buf = (batch + 1) % 2;\n",
    "            float* curr_input = d_input[curr_buf];\n",
    "            \n",
    "            // Async load next batch\n",
    "            if (batch + 1 < num_batches) {\n",
    "                cudaStreamSynchronize(stream_transfer);\n",
    "                memcpy(h_pinned_input, dataset.get_train_images_ptr() + (batch + 1) * s_in, s_in * 4);\n",
    "                cudaMemcpyAsync(d_input[next_buf], h_pinned_input, s_in * 4, \n",
    "                               cudaMemcpyHostToDevice, stream_transfer);\n",
    "            }\n",
    "            \n",
    "            if (batch == 0) cudaStreamSynchronize(stream_transfer);\n",
    "            \n",
    "            // ========== FORWARD (Fused GEMM+Bias+ReLU) ==========\n",
    "            // Layer 1: Conv + ReLU + MaxPool\n",
    "            im2col_kernel<<<GRID(col1_size), BLOCK_SIZE, 0, stream_compute>>>(\n",
    "                curr_input, d_col1, B, 32, 32, 3, 3, 1, 32, 32);\n",
    "            gemm_nt_bias_relu(d_col1, d_w1, d_b1, d_l1, B * 32 * 32, 3 * 9, 256, true, stream_compute);\n",
    "            maxpool_kernel<<<GRID(s_p1), BLOCK_SIZE, 0, stream_compute>>>(d_l1, d_p1, d_idx1, B, 32, 32, 256);\n",
    "            \n",
    "            // Layer 2: Conv + ReLU + MaxPool\n",
    "            im2col_kernel<<<GRID(col2_size), BLOCK_SIZE, 0, stream_compute>>>(\n",
    "                d_p1, d_col2, B, 16, 16, 256, 3, 1, 16, 16);\n",
    "            gemm_nt_bias_relu(d_col2, d_w2, d_b2, d_l2, B * 16 * 16, 256 * 9, 128, true, stream_compute);\n",
    "            maxpool_kernel<<<GRID(s_p2), BLOCK_SIZE, 0, stream_compute>>>(d_l2, d_p2, d_idx2, B, 16, 16, 128);\n",
    "            \n",
    "            // Layer 3: Conv + ReLU + Upsample\n",
    "            im2col_kernel<<<GRID(col3_size), BLOCK_SIZE, 0, stream_compute>>>(\n",
    "                d_p2, d_col3, B, 8, 8, 128, 3, 1, 8, 8);\n",
    "            gemm_nt_bias_relu(d_col3, d_w3, d_b3, d_l3, B * 8 * 8, 128 * 9, 128, true, stream_compute);\n",
    "            upsample_kernel<<<GRID(s_u3), BLOCK_SIZE, 0, stream_compute>>>(d_l3, d_u3, B, 8, 8, 128);\n",
    "            \n",
    "            // Layer 4: Conv + ReLU + Upsample\n",
    "            im2col_kernel<<<GRID(col4_size), BLOCK_SIZE, 0, stream_compute>>>(\n",
    "                d_u3, d_col4, B, 16, 16, 128, 3, 1, 16, 16);\n",
    "            gemm_nt_bias_relu(d_col4, d_w4, d_b4, d_l4, B * 16 * 16, 128 * 9, 256, true, stream_compute);\n",
    "            upsample_kernel<<<GRID(s_u4), BLOCK_SIZE, 0, stream_compute>>>(d_l4, d_u4, B, 16, 16, 256);\n",
    "            \n",
    "            // Layer 5: Conv (no ReLU)\n",
    "            im2col_kernel<<<GRID(col5_size), BLOCK_SIZE, 0, stream_compute>>>(\n",
    "                d_u4, d_col5, B, 32, 32, 256, 3, 1, 32, 32);\n",
    "            gemm_nt_bias_relu(d_col5, d_w5, d_b5, d_out, B * 32 * 32, 256 * 9, 3, false, stream_compute);\n",
    "            \n",
    "            // ========== FUSED LOSS + BACKWARD ==========\n",
    "            mse_loss_backward_fused_kernel<<<256, 256, 0, stream_compute>>>(\n",
    "                d_out, curr_input, d_dout, d_loss, s_in);\n",
    "            \n",
    "            // ========== BACKWARD WITH FUSED KERNELS ==========\n",
    "            \n",
    "            // Layer 5 backward (no ReLU - use standard kernels)\n",
    "            gemm_nn(d_dout, d_w5, d_dcol, B * 32 * 32, 3, 256 * 9, stream_compute);\n",
    "            col2im_kernel<<<GRID(s_u4), BLOCK_SIZE, 0, stream_compute>>>(\n",
    "                d_dcol, d_du4, B, 32, 32, 256, 3, 1, 32, 32);\n",
    "            gemm_tn(d_dout, d_col5, d_dw5, 3, B * 32 * 32, 256 * 9, stream_compute);\n",
    "            bias_backward_kernel<<<3, BLOCK_SIZE, 0, stream_compute>>>(d_dout, d_db5, B * 32 * 32, 3);\n",
    "            \n",
    "            // Layer 4 backward (FUSED: upsample + relu backward)\n",
    "            fused_upsample_relu_backward(d_du4, d_l4, d_dl4, B, 16, 16, 256, stream_compute);\n",
    "            gemm_nn(d_dl4, d_w4, d_dcol, B * 16 * 16, 256, 128 * 9, stream_compute);\n",
    "            col2im_kernel<<<GRID(s_u3), BLOCK_SIZE, 0, stream_compute>>>(\n",
    "                d_dcol, d_du3, B, 16, 16, 128, 3, 1, 16, 16);\n",
    "            gemm_tn(d_dl4, d_col4, d_dw4, 256, B * 16 * 16, 128 * 9, stream_compute);\n",
    "            bias_backward_kernel<<<256, BLOCK_SIZE, 0, stream_compute>>>(d_dl4, d_db4, B * 16 * 16, 256);\n",
    "            \n",
    "            // Layer 3 backward (FUSED: upsample + relu backward)\n",
    "            fused_upsample_relu_backward(d_du3, d_l3, d_dl3, B, 8, 8, 128, stream_compute);\n",
    "            gemm_nn(d_dl3, d_w3, d_dcol, B * 8 * 8, 128, 128 * 9, stream_compute);\n",
    "            col2im_kernel<<<GRID(s_p2), BLOCK_SIZE, 0, stream_compute>>>(\n",
    "                d_dcol, d_dp2, B, 8, 8, 128, 3, 1, 8, 8);\n",
    "            gemm_tn(d_dl3, d_col3, d_dw3, 128, B * 8 * 8, 128 * 9, stream_compute);\n",
    "            bias_backward_kernel<<<128, BLOCK_SIZE, 0, stream_compute>>>(d_dl3, d_db3, B * 8 * 8, 128);\n",
    "            \n",
    "            // Layer 2 backward (FUSED: zero + maxpool + relu backward)\n",
    "            fused_maxpool_relu_backward(d_dp2, d_idx2, d_l2, d_dl2, s_p2, s_l2, stream_compute);\n",
    "            gemm_nn(d_dl2, d_w2, d_dcol, B * 16 * 16, 128, 256 * 9, stream_compute);\n",
    "            col2im_kernel<<<GRID(s_p1), BLOCK_SIZE, 0, stream_compute>>>(\n",
    "                d_dcol, d_dp1, B, 16, 16, 256, 3, 1, 16, 16);\n",
    "            gemm_tn(d_dl2, d_col2, d_dw2, 128, B * 16 * 16, 256 * 9, stream_compute);\n",
    "            bias_backward_kernel<<<128, BLOCK_SIZE, 0, stream_compute>>>(d_dl2, d_db2, B * 16 * 16, 128);\n",
    "            \n",
    "            // Layer 1 backward (FUSED: zero + maxpool + relu backward)\n",
    "            fused_maxpool_relu_backward(d_dp1, d_idx1, d_l1, d_dl1, s_p1, s_l1, stream_compute);\n",
    "            gemm_tn(d_dl1, d_col1, d_dw1, 256, B * 32 * 32, 3 * 9, stream_compute);\n",
    "            bias_backward_kernel<<<256, BLOCK_SIZE, 0, stream_compute>>>(d_dl1, d_db1, B * 32 * 32, 256);\n",
    "            \n",
    "            // ========== SGD UPDATE (Vectorized) ==========\n",
    "            sgd_vectorized_kernel<<<GRID(h_w1.size() / 4), BLOCK_SIZE, 0, stream_compute>>>(\n",
    "                d_w1, d_dw1, h_w1.size(), LR);\n",
    "            sgd_vectorized_kernel<<<GRID(256 / 4), BLOCK_SIZE, 0, stream_compute>>>(\n",
    "                d_b1, d_db1, 256, LR);\n",
    "            sgd_vectorized_kernel<<<GRID(h_w2.size() / 4), BLOCK_SIZE, 0, stream_compute>>>(\n",
    "                d_w2, d_dw2, h_w2.size(), LR);\n",
    "            sgd_vectorized_kernel<<<GRID(128 / 4), BLOCK_SIZE, 0, stream_compute>>>(\n",
    "                d_b2, d_db2, 128, LR);\n",
    "            sgd_vectorized_kernel<<<GRID(h_w3.size() / 4), BLOCK_SIZE, 0, stream_compute>>>(\n",
    "                d_w3, d_dw3, h_w3.size(), LR);\n",
    "            sgd_vectorized_kernel<<<GRID(128 / 4), BLOCK_SIZE, 0, stream_compute>>>(\n",
    "                d_b3, d_db3, 128, LR);\n",
    "            sgd_vectorized_kernel<<<GRID(h_w4.size() / 4), BLOCK_SIZE, 0, stream_compute>>>(\n",
    "                d_w4, d_dw4, h_w4.size(), LR);\n",
    "            sgd_vectorized_kernel<<<GRID(256 / 4), BLOCK_SIZE, 0, stream_compute>>>(\n",
    "                d_b4, d_db4, 256, LR);\n",
    "            sgd_vectorized_kernel<<<GRID(h_w5.size() / 4), BLOCK_SIZE, 0, stream_compute>>>(\n",
    "                d_w5, d_dw5, h_w5.size(), LR);\n",
    "            sgd_kernel<<<GRID(3), BLOCK_SIZE, 0, stream_compute>>>(d_b5, d_db5, 3, LR);\n",
    "        }\n",
    "        \n",
    "        float h_loss;\n",
    "        cudaMemcpyAsync(&h_loss, d_loss, 4, cudaMemcpyDeviceToHost, stream_compute);\n",
    "        cudaStreamSynchronize(stream_compute);\n",
    "        \n",
    "        auto ep_end = std::chrono::high_resolution_clock::now();\n",
    "        double ep_time = std::chrono::duration<double>(ep_end - ep_start).count();\n",
    "\n",
    "        // Print example weights from h_w1 after each epoch\n",
    "        std::cout << \"Epoch \" << epoch + 1 << \": Example weights from h_w1: \";\n",
    "        cudaMemcpy(h_w1.data(), d_w1, h_w1.size() * 4, cudaMemcpyDeviceToHost);\n",
    "        \n",
    "        // Print example weights from h_w1 after each epoch\n",
    "        std::cout << \"Epoch \" << epoch + 1 << \": Example weights from d_w1: \";\n",
    "        for (size_t i = 0; i < std::min((size_t)10, h_w1.size()); ++i) {\n",
    "            std::cout << std::fixed << std::setprecision(6) << h_w1[i] << \" \";\n",
    "        }\n",
    "        std::cout << std::endl;\n",
    "\n",
    "        std::cout << \"Epoch \" << (epoch + 1) << \"/\" << EPOCHS\n",
    "                  << \" | Loss: \" << std::fixed << std::setprecision(6) << h_loss / (num_batches * s_in)\n",
    "                  << \" | Time: \" << std::setprecision(2) << ep_time << \"s\"\n",
    "                  << \" | \" << std::setprecision(0) << (num_batches * B) / ep_time << \" img/s\\n\";\n",
    "    }\n",
    "    \n",
    "    auto t_end = std::chrono::high_resolution_clock::now();\n",
    "    std::cout << \"\\nTotal: \" << std::chrono::duration<double>(t_end - t_start).count() << \"s\\n\";\n",
    "    \n",
    "    // Save weights\n",
    "    cudaMemcpy(h_w1.data(), d_w1, h_w1.size() * 4, cudaMemcpyDeviceToHost);\n",
    "    cudaMemcpy(h_b1.data(), d_b1, 256 * 4, cudaMemcpyDeviceToHost);\n",
    "    cudaMemcpy(h_w2.data(), d_w2, h_w2.size() * 4, cudaMemcpyDeviceToHost);\n",
    "    cudaMemcpy(h_b2.data(), d_b2, 128 * 4, cudaMemcpyDeviceToHost);\n",
    "    cudaMemcpy(h_w3.data(), d_w3, h_w3.size() * 4, cudaMemcpyDeviceToHost);\n",
    "    cudaMemcpy(h_b3.data(), d_b3, 128 * 4, cudaMemcpyDeviceToHost);\n",
    "    cudaMemcpy(h_w4.data(), d_w4, h_w4.size() * 4, cudaMemcpyDeviceToHost);\n",
    "    cudaMemcpy(h_b4.data(), d_b4, 256 * 4, cudaMemcpyDeviceToHost);\n",
    "    cudaMemcpy(h_w5.data(), d_w5, h_w5.size() * 4, cudaMemcpyDeviceToHost);\n",
    "    cudaMemcpy(h_b5.data(), d_b5, 3 * 4, cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    system(\"mkdir -p ../weights\");\n",
    "    save_weights(\"../weights/enc_w1.bin\", h_w1); save_weights(\"../weights/enc_b1.bin\", h_b1);\n",
    "    save_weights(\"../weights/enc_w2.bin\", h_w2); save_weights(\"../weights/enc_b2.bin\", h_b2);\n",
    "    save_weights(\"../weights/dec_w3.bin\", h_w3); save_weights(\"../weights/dec_b3.bin\", h_b3);\n",
    "    save_weights(\"../weights/dec_w4.bin\", h_w4); save_weights(\"../weights/dec_b4.bin\", h_b4);\n",
    "    save_weights(\"../weights/dec_w5.bin\", h_w5); save_weights(\"../weights/dec_b5.bin\", h_b5);\n",
    "    \n",
    "    cudaFreeHost(h_pinned_input);\n",
    "    cudaStreamDestroy(stream_compute);\n",
    "    cudaStreamDestroy(stream_transfer);\n",
    "    std::cout << \"Saved weights.\\n\";\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8d5fba",
   "metadata": {
    "id": "JmNUVm57G0cm",
    "papermill": {
     "duration": 0.004836,
     "end_time": "2025-12-14T14:01:41.024140",
     "exception": false,
     "start_time": "2025-12-14T14:01:41.019304",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## train (phase 3 full image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb591b81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T14:01:41.035095Z",
     "iopub.status.busy": "2025-12-14T14:01:41.034513Z",
     "iopub.status.idle": "2025-12-14T14:01:41.155324Z",
     "shell.execute_reply": "2025-12-14T14:01:41.154412Z"
    },
    "id": "nsp2Qk-SGeS5",
    "outputId": "cb66eff2-03ae-47c7-972e-9cf0e9379a31",
    "papermill": {
     "duration": 0.127697,
     "end_time": "2025-12-14T14:01:41.156692",
     "exception": false,
     "start_time": "2025-12-14T14:01:41.028995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build  data  include  README.md  src  weights\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17e3ad6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T14:01:41.167824Z",
     "iopub.status.busy": "2025-12-14T14:01:41.167533Z",
     "iopub.status.idle": "2025-12-14T14:01:46.982256Z",
     "shell.execute_reply": "2025-12-14T14:01:46.981391Z"
    },
    "id": "NVqNogpjHPAs",
    "papermill": {
     "duration": 5.822369,
     "end_time": "2025-12-14T14:01:46.983873",
     "exception": false,
     "start_time": "2025-12-14T14:01:41.161504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_75 -o build/train_gpu_optimize src/train_gpu_optimize.cu src/cifar10_dataset.cpp -I include/\n",
    "# !nvcc src/train_gpu_optimize.cu src/cifar10_dataset.cpp -o build/train_gpu_optimize -O3 -use_fast_math -arch=sm_75 -lcuda -lcudart -I include/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d921a0eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T14:01:46.995269Z",
     "iopub.status.busy": "2025-12-14T14:01:46.995020Z",
     "iopub.status.idle": "2025-12-14T14:42:20.066432Z",
     "shell.execute_reply": "2025-12-14T14:42:20.065274Z"
    },
    "id": "W-ai-RhzHSAR",
    "outputId": "8702e562-7b85-4d6f-a08b-b1ea8fd480b3",
    "papermill": {
     "duration": 2433.079039,
     "end_time": "2025-12-14T14:42:20.068033",
     "exception": false,
     "start_time": "2025-12-14T14:01:46.988994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/Autoencoder-based-unsupervised-feature-learning-system/build\n",
      "=== CUDA Autoencoder (Fused Backward Kernels) ===\r\n",
      "\r\n",
      "GPU: Tesla T4\r\n",
      "SMs: 40\r\n",
      "\r\n",
      "--- Loading CIFAR-10 Dataset ---\r\n",
      "Loaded batch: ../data/cifar-10-batches-bin/data_batch_1.bin | Current Total: 10000\r\n",
      "Loaded batch: ../data/cifar-10-batches-bin/data_batch_2.bin | Current Total: 20000\r\n",
      "Loaded batch: ../data/cifar-10-batches-bin/data_batch_3.bin | Current Total: 30000\r\n",
      "Loaded batch: ../data/cifar-10-batches-bin/data_batch_4.bin | Current Total: 40000\r\n",
      "Loaded batch: ../data/cifar-10-batches-bin/data_batch_5.bin | Current Total: 50000\r\n",
      "Loaded batch: ../data/cifar-10-batches-bin/test_batch.bin | Current Total: 10000\r\n",
      "Successfully loaded 50000 train images and 10000 test images.\r\n",
      "Images: 50000\r\n",
      "\r\n",
      "Memory: 1779 MB\r\n",
      "\r\n",
      "Training: 20 epochs, 781 batches\r\n",
      "\r\n",
      "Epoch 1/20 | Loss: 0.109956 | Time: 119.41s | 419 img/s\r\n",
      "Epoch 2/20 | Loss: 0.085264 | Time: 121.62s | 411 img/s\r\n",
      "Epoch 3/20 | Loss: 0.083649 | Time: 121.77s | 410 img/s\r\n",
      "Epoch 4/20 | Loss: 0.082535 | Time: 121.40s | 412 img/s\r\n",
      "Epoch 5/20 | Loss: 0.081319 | Time: 121.39s | 412 img/s\r\n",
      "Epoch 6/20 | Loss: 0.079829 | Time: 121.49s | 411 img/s\r\n",
      "Epoch 7/20 | Loss: 0.078113 | Time: 121.19s | 412 img/s\r\n",
      "Epoch 8/20 | Loss: 0.076819 | Time: 121.22s | 412 img/s\r\n",
      "Epoch 9/20 | Loss: 0.076095 | Time: 121.29s | 412 img/s\r\n",
      "Epoch 10/20 | Loss: 0.075792 | Time: 121.35s | 412 img/s\r\n",
      "Epoch 11/20 | Loss: 0.075770 | Time: 121.15s | 413 img/s\r\n",
      "Epoch 12/20 | Loss: 0.075994 | Time: 121.17s | 413 img/s\r\n",
      "Epoch 13/20 | Loss: 0.076411 | Time: 121.11s | 413 img/s\r\n",
      "Epoch 14/20 | Loss: 0.076899 | Time: 121.00s | 413 img/s\r\n",
      "Epoch 15/20 | Loss: 0.077345 | Time: 121.00s | 413 img/s\r\n",
      "Epoch 16/20 | Loss: 0.077661 | Time: 121.02s | 413 img/s\r\n",
      "Epoch 17/20 | Loss: 0.077780 | Time: 120.98s | 413 img/s\r\n",
      "Epoch 18/20 | Loss: 0.077729 | Time: 120.89s | 413 img/s\r\n",
      "Epoch 19/20 | Loss: 0.077558 | Time: 121.01s | 413 img/s\r\n",
      "Epoch 20/20 | Loss: 0.077155 | Time: 121.01s | 413 img/s\r\n",
      "\r\n",
      "Total: 2422s\r\n",
      "Saved weights.\r\n",
      "/kaggle/working/Autoencoder-based-unsupervised-feature-learning-system\n"
     ]
    }
   ],
   "source": [
    "%cd build/\n",
    "!./train_gpu_optimize\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c79716",
   "metadata": {
    "id": "ZP0wA_zmyiD7",
    "papermill": {
     "duration": 0.005781,
     "end_time": "2025-12-14T14:42:20.080519",
     "exception": false,
     "start_time": "2025-12-14T14:42:20.074738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2452.392965,
   "end_time": "2025-12-14T14:42:20.503722",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-14T14:01:28.110757",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
